{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fd14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db67757b",
   "metadata": {},
   "source": [
    "## Read Tolstoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137247d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tolstoy2.txt\",encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30b1231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Л.Н. ТОЛСТОЙ\n",
      "    ВОЙНА И МИР\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ТОМ ПЕРВЫЙ\n",
      "\n",
      "\n",
      "ЧАСТЬ ПЕРВАЯ\n",
      "\n",
      "I.\n",
      "\n",
      "   - Еh bien, mon prince. Gкnes et Lucques ne sont plus  que  des  apanages,\n",
      "des поместья, de la famille Buonaparte. Non, je vous prйviens, que  si  vous\n",
      "ne me dites pas, que nous avons la guerre, si vous vous permettez encore  de\n",
      "pallier toutes les infamies, toutes les  atrocitйs  de  cet  Antichrist  (ma\n",
      "parole, j'y crois) - je ne vous connais plus, vous n'кtes plus mon ami, vous\n",
      "n'кtes plus мой верный раб, comme vous dites. 1 (См. сноски в  конце  части)\n",
      "Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, 2 садитесь  и\n",
      "рассказывайте.\n",
      "   Так говорила в июле 1805 года известная Анна Павловна Шерер,  фрейлина  и\n",
      "приближенная императрицы Марии  Феодоровны,  встречая  важного  и  чиновного\n",
      "князя Василия, первого  приехавшего  на  ее  вечер.  Анна  Павловна  кашляла\n",
      "несколько дней, у нее был грипп, как она говорила  (грипп  был  тогда  новое\n",
      "слово, употреблявшееся только редкими). В записочках,  разосланных  у\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c376c8",
   "metadata": {},
   "source": [
    "# Оставляет только русские символы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fb3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"\"\n",
    "\n",
    "russian_chars = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ\\n.,!?;:-()\\\"' \"\n",
    "for symbol in text:\n",
    "    if symbol in russian_chars:\n",
    "        new_text+= symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b2a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = new_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f61031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    л.н. толстой\n",
      "    война и мир\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "том первый\n",
      "\n",
      "\n",
      "часть первая\n",
      "\n",
      ".\n",
      "\n",
      "   - е ,  . к           ,\n",
      " поместья,    . ,   й,     \n",
      "   ,     ,       \n",
      "   ,    й        (\n",
      ", ' ) -     ,  'к   , \n",
      "'к  мой верный раб,   .  (см. сноски в  конце  части)\n",
      "ну, здравствуйте, здравствуйте.       ,  садитесь  и\n",
      "рассказывайте.\n",
      "   так говорила в июле  года известная анна павловна шерер,  фрейлина  и\n",
      "приближенная императрицы марии  феодоровны,  встречая  важного  и  чиновного\n",
      "князя василия, первого  приехавшего  на  ее  вечер.  анна  павловна  кашляла\n",
      "несколько дней, у нее был грипп, как она говорила  (грипп  был  тогда  новое\n",
      "слово, употреблявшееся только редкими). в записочках,  разосланных  утром  с\n",
      "красным лакеем, было написано без различия во всех:\n",
      "   \"  '    а , .   (или   ),  \n",
      "      й       \n",
      " ,   й                .\n",
      " \".\n",
      "   - ,     - отвечал, нисколько не  смутясь  такою\n",
      "встречей, вошедший князь, в придворном, шитом мундире, в  чулках,  башмаках,\n",
      "при  звездах,  с  светлым  выражением  плоского  лица.  он  го\n"
     ]
    }
   ],
   "source": [
    "print(new_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60809517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё']\n",
      "len: 46\n"
     ]
    }
   ],
   "source": [
    "text = new_text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(f\"len: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd7b047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 29, 21, 15, 18, 31, 1, 23, 13, 23, 1, 17, 18, 24, 13, 12]\n",
      "привет как дела?\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "# string to integer\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "## integer to string\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # кодировщик: берем строку, выводим список целых чисел\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) #декодер: берем список целых чисел, выводим строку\n",
    "\n",
    "print(encode(\"привет как дела?\"))\n",
    "print(decode(encode(\"привет как дела?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6634190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3073006]) torch.int64\n",
      "tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22, 26, 13,  1, 21,  1, 25, 21, 29,  0,  0,  0,  0,\n",
      "         0, 31, 27, 25,  1, 28, 18, 29, 15, 40, 22,  0,  0,  0, 36, 13, 30, 31,\n",
      "        41,  1, 28, 18, 29, 15, 13, 44,  0,  0,  9,  0,  0,  1,  1,  1,  8,  1,\n",
      "        18,  1,  7,  1,  1,  9,  1, 23,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  7,  0,  1, 28, 27, 25, 18, 30, 31, 41, 44,  7,  1,  1,  1,  1,  9,\n",
      "         1,  7,  1,  1,  1, 22,  7,  1,  1,  1,  1,  1,  0,  1,  1,  1,  7,  1,\n",
      "         1,  1,  1,  1,  7,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,  7,  1,\n",
      "         1,  1,  1, 22,  1,  1,  1,  1,  1,  1,  1,  1,  5,  0,  7,  1,  4,  1,\n",
      "         6,  1,  8,  1,  1,  1,  1,  1,  7,  1,  1,  4, 23,  1,  1,  1,  7,  1,\n",
      "         0,  4, 23,  1,  1, 25, 27, 22,  1, 15, 18, 29, 26, 40, 22,  1, 29, 13,\n",
      "        14,  7,  1,  1,  1,  9,  1,  1,  5, 30, 25,  9,  1, 30, 26, 27, 30, 23,\n",
      "        21,  1, 15,  1,  1, 23, 27, 26, 35, 18,  1,  1, 36, 13, 30, 31, 21,  6,\n",
      "         0, 26, 32,  7,  1, 20, 17, 29, 13, 15, 30, 31, 15, 32, 22, 31, 18,  7,\n",
      "         1, 20, 17, 29, 13, 15, 30, 31, 15, 32, 22, 31, 18,  9,  1,  1,  1,  1,\n",
      "         1,  1,  1,  7,  1,  1, 30, 13, 17, 21, 31, 18, 30, 41,  1,  1, 21,  0,\n",
      "        29, 13, 30, 30, 23, 13, 20, 40, 15, 13, 22, 31, 18,  9,  0,  1,  1,  1,\n",
      "        31, 13, 23,  1, 16, 27, 15, 27, 29, 21, 24, 13,  1, 15,  1, 21, 43, 24,\n",
      "        18,  1,  1, 16, 27, 17, 13,  1, 21, 20, 15, 18, 30, 31, 26, 13, 44,  1,\n",
      "        13, 26, 26, 13,  1, 28, 13, 15, 24, 27, 15, 26, 13,  1, 37, 18, 29, 18,\n",
      "        29,  7,  1,  1, 33, 29, 18, 22, 24, 21, 26, 13,  1,  1, 21,  0, 28, 29,\n",
      "        21, 14, 24, 21, 19, 18, 26, 26, 13, 44,  1, 21, 25, 28, 18, 29, 13, 31,\n",
      "        29, 21, 35, 40,  1, 25, 13, 29, 21, 21,  1,  1, 33, 18, 27, 17, 27, 29,\n",
      "        27, 15, 26, 40,  7,  1,  1, 15, 30, 31, 29, 18, 36, 13, 44,  1,  1, 15,\n",
      "        13, 19, 26, 27, 16, 27,  1,  1, 21,  1,  1, 36, 21, 26, 27, 15, 26, 27,\n",
      "        16, 27,  0, 23, 26, 44, 20, 44,  1, 15, 13, 30, 21, 24, 21, 44,  7,  1,\n",
      "        28, 18, 29, 15, 27, 16, 27,  1,  1, 28, 29, 21, 18, 34, 13, 15, 37, 18,\n",
      "        16, 27,  1,  1, 26, 13,  1,  1, 18, 18,  1,  1, 15, 18, 36, 18, 29,  9,\n",
      "         1,  1, 13, 26, 26, 13,  1,  1, 28, 13, 15, 24, 27, 15, 26, 13,  1,  1,\n",
      "        23, 13, 37, 24, 44, 24, 13,  0, 26, 18, 30, 23, 27, 24, 41, 23, 27,  1,\n",
      "        17, 26, 18, 22,  7,  1, 32,  1, 26, 18, 18,  1, 14, 40, 24,  1, 16, 29,\n",
      "        21, 28, 28,  7,  1, 23, 13, 23,  1, 27, 26, 13,  1, 16, 27, 15, 27, 29,\n",
      "        21, 24, 13,  1,  1,  5, 16, 29, 21, 28, 28,  1,  1, 14, 40, 24,  1,  1,\n",
      "        31, 27, 16, 17, 13,  1,  1, 26, 27, 15, 27, 18,  0, 30, 24, 27, 15, 27,\n",
      "         7,  1, 32, 28, 27, 31, 29, 18, 14, 24, 44, 15, 37, 18, 18, 30, 44,  1,\n",
      "        31, 27, 24, 41, 23, 27,  1, 29, 18, 17, 23, 21, 25, 21,  6,  9,  1, 15,\n",
      "         1, 20, 13, 28, 21, 30, 27, 36, 23, 13, 34,  7,  1,  1, 29, 13, 20, 27,\n",
      "        30, 24, 13, 26, 26, 40, 34,  1,  1, 32, 31, 29, 27, 25,  1,  1, 30,  0,\n",
      "        23, 29, 13, 30, 26, 40, 25,  1, 24, 13, 23, 18, 18, 25,  7,  1, 14, 40,\n",
      "        24, 27,  1, 26, 13, 28, 21, 30, 13, 26, 27,  1, 14, 18, 20,  1, 29, 13,\n",
      "        20, 24, 21, 36, 21, 44,  1, 15, 27,  1, 15, 30, 18, 34, 10,  0,  1,  1,\n",
      "         1,  3,  1,  1,  4,  1,  1,  1,  1, 13,  1,  7,  1,  9,  1,  1,  1,  5,\n",
      "        21, 24, 21,  1,  1,  1,  6,  7,  1,  1,  0,  1,  1,  1,  1,  1,  1, 22,\n",
      "         1,  1,  1,  1,  1,  1,  1,  0,  1,  7,  1,  1,  1, 22,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  9,  0,  1,  3,  9,  0,\n",
      "         1,  1,  1,  8,  1,  7,  1,  1,  1,  1,  1,  8,  1, 27, 31, 15, 18, 36,\n",
      "        13, 24,  7,  1, 26, 21, 30, 23, 27, 24, 41, 23, 27,  1, 26, 18,  1,  1,\n",
      "        30, 25, 32, 31, 44, 30, 41,  1,  1, 31, 13, 23, 27, 43,  0, 15, 30, 31,\n",
      "        29, 18, 36, 18, 22,  7,  1, 15, 27, 37, 18, 17, 37, 21, 22,  1, 23, 26,\n",
      "        44, 20, 41,  7,  1, 15,  1, 28, 29, 21, 17, 15, 27, 29, 26, 27, 25,  7,\n",
      "         1, 37, 21, 31, 27, 25,  1, 25, 32, 26, 17, 21, 29, 18,  7,  1, 15,  1,\n",
      "         1, 36, 32, 24, 23, 13, 34,  7,  1,  1, 14, 13, 37, 25, 13, 23, 13, 34,\n",
      "         7,  0, 28, 29, 21,  1,  1, 20, 15, 18, 20, 17, 13, 34,  7,  1,  1, 30,\n",
      "         1,  1, 30, 15, 18, 31, 24, 40, 25,  1,  1, 15, 40, 29, 13, 19, 18, 26,\n",
      "        21, 18, 25,  1,  1, 28, 24, 27, 30, 23, 27, 16, 27,  1,  1, 24, 21, 35,\n",
      "        13,  9,  1,  1, 27, 26,  1,  1, 16, 27])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # 1000 символов, которые мы просмотрели ранее, в GPT будут выглядеть следующим образомs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ec2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте теперь разделим данные на обучающие и проверочные наборы\n",
    "n = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98883862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1243b4030>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64 ### ембединги токена\n",
    "n_head = 4 ## кол голов\n",
    "n_layer = 4 ## кол во слоев\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a344e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "когда на входе tensor([1]) выход вот такой: 1\n",
      "когда на входе tensor([1, 1]) выход вот такой: 1\n",
      "когда на входе tensor([1, 1, 1]) выход вот такой: 1\n",
      "когда на входе tensor([1, 1, 1, 1]) выход вот такой: 24\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24]) выход вот такой: 9\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9]) выход вот такой: 26\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26]) выход вот такой: 9\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1]) выход вот такой: 31\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31]) выход вот такой: 27\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27]) выход вот такой: 24\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24]) выход вот такой: 30\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30]) выход вот такой: 31\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31]) выход вот такой: 27\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27]) выход вот такой: 22\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22]) выход вот такой: 0\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1]) выход вот такой: 15\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15]) выход вот такой: 27\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27]) выход вот такой: 22\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22]) выход вот такой: 26\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22, 26]) выход вот такой: 13\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22, 26, 13]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22, 26, 13,  1]) выход вот такой: 21\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22, 26, 13,  1, 21]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22, 26, 13,  1, 21,  1]) выход вот такой: 25\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22, 26, 13,  1, 21,  1, 25]) выход вот такой: 21\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22, 26, 13,  1, 21,  1, 25, 21]) выход вот такой: 29\n",
      "когда на входе tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22, 26, 13,  1, 21,  1, 25, 21, 29]) выход вот такой: 0\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "\n",
    "    contex = x[:t + 1]\n",
    "    target = y[t]\n",
    "\n",
    "    print(f\"когда на входе {contex} выход вот такой: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1bc6131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([16, 32])\n",
      "tensor([[29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "         31, 18, 22,  1,  1, 14, 18, 20,  1,  1, 27, 30, 13, 17],\n",
      "        [ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "         30, 18, 16, 17, 13,  7,  1, 36, 31, 27,  1, 27, 26, 21],\n",
      "        [17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "         13, 24, 13, 30, 41,  9,  1,  1,  8,  1,  1,  3, 17, 13],\n",
      "        [18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "         15, 18, 18,  1,  1, 21,  1,  1, 28, 18, 36, 13, 24, 41],\n",
      "        [27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "         15, 26, 32, 31, 29, 21,  1, 23, 13, 29, 18, 31, 40,  7],\n",
      "        [30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "         31, 21, 15, 37, 21, 30, 41,  1, 21, 20,  1, 30, 15, 27],\n",
      "        [ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "         21, 23, 27, 24, 13, 22,  1,  1, 15, 21, 17, 18, 24,  7],\n",
      "        [13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "          7,  1, 28, 29, 21, 15, 40, 36, 26, 40, 25,  1, 19, 18],\n",
      "        [ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "         29, 21, 30,  7,  1,  1, 19, 18, 24, 13, 44,  1,  1, 30],\n",
      "        [30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "         21,  1, 35, 13, 29, 18, 22,  1, 28, 27, 24, 41, 20, 32],\n",
      "        [40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "         25, 13, 26, 18, 29, 27, 22,  1,  1, 15, 20, 44, 24,  1],\n",
      "        [ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "          1, 30, 23, 13, 20, 13, 24, 12,  1, 26, 13, 31, 13, 37],\n",
      "        [25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "          1,  1, 28, 27, 17, 15, 27, 17, 40,  9,  0, 15,  1, 30],\n",
      "        [ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "          9,  9,  9,  1, 16, 24, 32, 28, 27,  2,  1, 30, 25, 18],\n",
      "        [13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "         13, 23,  7,  1, 36, 31, 27,  1, 27, 26,  1, 26, 21, 36],\n",
      "        [21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "         13, 43, 31, 30, 44,  1,  1, 17, 27, 16, 13, 17, 13, 31]])\n",
      "targets:\n",
      "torch.Size([16, 32])\n",
      "tensor([[13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30, 31,\n",
      "         18, 22,  1,  1, 14, 18, 20,  1,  1, 27, 30, 13, 17, 40],\n",
      "        [ 0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15, 30,\n",
      "         18, 16, 17, 13,  7,  1, 36, 31, 27,  1, 27, 26, 21,  1],\n",
      "        [24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36, 13,\n",
      "         24, 13, 30, 41,  9,  1,  1,  8,  1,  1,  3, 17, 13, 24],\n",
      "        [16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21, 15,\n",
      "         18, 18,  1,  1, 21,  1,  1, 28, 18, 36, 13, 24, 41, 26],\n",
      "        [36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1, 15,\n",
      "         26, 32, 31, 29, 21,  1, 23, 13, 29, 18, 31, 40,  7,  1],\n",
      "        [31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13, 31,\n",
      "         21, 15, 37, 21, 30, 41,  1, 21, 20,  1, 30, 15, 27, 18],\n",
      "        [ 1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26, 21,\n",
      "         23, 27, 24, 13, 22,  1,  1, 15, 21, 17, 18, 24,  7,  1],\n",
      "        [19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,  7,\n",
      "          1, 28, 29, 21, 15, 40, 36, 26, 40, 25,  1, 19, 18, 30],\n",
      "        [ 9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27, 29,\n",
      "         21, 30,  7,  1,  1, 19, 18, 24, 13, 44,  1,  1, 30, 23],\n",
      "        [44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26, 21,\n",
      "          1, 35, 13, 29, 18, 22,  1, 28, 27, 24, 41, 20, 32, 18],\n",
      "        [30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1, 25,\n",
      "         13, 26, 18, 29, 27, 22,  1,  1, 15, 20, 44, 24,  1,  1],\n",
      "        [ 1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,  1,\n",
      "         30, 23, 13, 20, 13, 24, 12,  1, 26, 13, 31, 13, 37, 13],\n",
      "        [21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,  1,\n",
      "          1, 28, 27, 17, 15, 27, 17, 40,  9,  0, 15,  1, 30, 27],\n",
      "        [30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,  9,\n",
      "          9,  9,  1, 16, 24, 32, 28, 27,  2,  1, 30, 25, 18, 29],\n",
      "        [17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31, 13,\n",
      "         23,  7,  1, 36, 31, 27,  1, 27, 26,  1, 26, 21, 36, 18],\n",
      "        [25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29, 13,\n",
      "         43, 31, 30, 44,  1,  1, 17, 27, 16, 13, 17, 13, 31, 41]])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ef109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "когда на входе tensor([29]) выход вот такой: 13\n",
      "когда на входе tensor([29, 13]) выход вот такой: 19\n",
      "когда на входе tensor([29, 13, 19]) выход вот такой: 18\n",
      "когда на входе tensor([29, 13, 19, 18]) выход вот такой: 26\n",
      "когда на входе tensor([29, 13, 19, 18, 26]) выход вот такой: 21\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21]) выход вот такой: 22\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22]) выход вот такой: 1\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1]) выход вот такой: 1\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1]) выход вот такой: 21\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21]) выход вот такой: 1\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1]) выход вот такой: 1\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1]) выход вот такой: 23\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23]) выход вот такой: 29\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29]) выход вот такой: 18\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18]) выход вот такой: 28\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28]) выход вот такой: 27\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27]) выход вот такой: 30\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30]) выход вот такой: 31\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31]) выход вот такой: 18\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18]) выход вот такой: 22\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22]) выход вот такой: 1\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1]) выход вот такой: 1\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1,  1]) выход вот такой: 14\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1,  1, 14]) выход вот такой: 18\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1,  1, 14, 18]) выход вот такой: 20\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1,  1, 14, 18, 20]) выход вот такой: 1\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1,  1, 14, 18, 20,  1]) выход вот такой: 1\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1,  1, 14, 18, 20,  1,  1]) выход вот такой: 27\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1,  1, 14, 18, 20,  1,  1, 27]) выход вот такой: 30\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1,  1, 14, 18, 20,  1,  1, 27, 30]) выход вот такой: 13\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1,  1, 14, 18, 20,  1,  1, 27, 30, 13]) выход вот такой: 17\n",
      "когда на входе tensor([29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "        31, 18, 22,  1,  1, 14, 18, 20,  1,  1, 27, 30, 13, 17]) выход вот такой: 40\n",
      "когда на входе tensor([7]) выход вот такой: 0\n",
      "когда на входе tensor([7, 0]) выход вот такой: 29\n",
      "когда на входе tensor([ 7,  0, 29]) выход вот такой: 18\n",
      "когда на входе tensor([ 7,  0, 29, 18]) выход вот такой: 37\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37]) выход вот такой: 21\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21]) выход вот такой: 15\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15]) выход вот такой: 37\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37]) выход вот такой: 21\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21]) выход вот такой: 18\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18]) выход вот такой: 1\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1]) выход вот такой: 29\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29]) выход вот такой: 13\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13]) выход вот такой: 20\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20]) выход вот такой: 1\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1]) выход вот такой: 26\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26]) выход вот такой: 13\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13]) выход вот такой: 15\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15]) выход вот такой: 30\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30]) выход вот такой: 18\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18]) выход вот такой: 16\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16]) выход вот такой: 17\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17]) выход вот такой: 13\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17, 13]) выход вот такой: 7\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17, 13,  7]) выход вот такой: 1\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17, 13,  7,  1]) выход вот такой: 36\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17, 13,  7,  1, 36]) выход вот такой: 31\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17, 13,  7,  1, 36, 31]) выход вот такой: 27\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17, 13,  7,  1, 36, 31, 27]) выход вот такой: 1\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17, 13,  7,  1, 36, 31, 27,  1]) выход вот такой: 27\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17, 13,  7,  1, 36, 31, 27,  1, 27]) выход вот такой: 26\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17, 13,  7,  1, 36, 31, 27,  1, 27, 26]) выход вот такой: 21\n",
      "когда на входе tensor([ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "        30, 18, 16, 17, 13,  7,  1, 36, 31, 27,  1, 27, 26, 21]) выход вот такой: 1\n",
      "когда на входе tensor([17]) выход вот такой: 24\n",
      "когда на входе tensor([17, 24]) выход вот такой: 44\n",
      "когда на входе tensor([17, 24, 44]) выход вот такой: 1\n",
      "когда на входе tensor([17, 24, 44,  1]) выход вот такой: 26\n",
      "когда на входе tensor([17, 24, 44,  1, 26]) выход вот такой: 18\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18]) выход вот такой: 16\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16]) выход вот такой: 27\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27]) выход вот такой: 1\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1]) выход вот такой: 32\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32]) выход вот такой: 19\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19]) выход вот такой: 18\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18]) выход вот такой: 1\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1]) выход вот такой: 1\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1]) выход вот такой: 23\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23]) выход вот такой: 27\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27]) выход вот такой: 26\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26]) выход вот такой: 36\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36]) выход вот такой: 13\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13]) выход вот такой: 24\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24]) выход вот такой: 13\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13]) выход вот такой: 30\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30]) выход вот такой: 41\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30, 41]) выход вот такой: 9\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30, 41,  9]) выход вот такой: 1\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30, 41,  9,  1]) выход вот такой: 1\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30, 41,  9,  1,  1]) выход вот такой: 8\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30, 41,  9,  1,  1,  8]) выход вот такой: 1\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30, 41,  9,  1,  1,  8,  1]) выход вот такой: 1\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30, 41,  9,  1,  1,  8,  1,  1]) выход вот такой: 3\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30, 41,  9,  1,  1,  8,  1,  1,  3]) выход вот такой: 17\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30, 41,  9,  1,  1,  8,  1,  1,  3, 17]) выход вот такой: 13\n",
      "когда на входе tensor([17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "        13, 24, 13, 30, 41,  9,  1,  1,  8,  1,  1,  3, 17, 13]) выход вот такой: 24\n",
      "когда на входе tensor([18]) выход вот такой: 16\n",
      "когда на входе tensor([18, 16]) выход вот такой: 27\n",
      "когда на входе tensor([18, 16, 27]) выход вот такой: 1\n",
      "когда на входе tensor([18, 16, 27,  1]) выход вот такой: 26\n",
      "когда на входе tensor([18, 16, 27,  1, 26]) выход вот такой: 18\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18]) выход вот такой: 31\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31]) выход вот такой: 1\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1]) выход вот такой: 1\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1]) выход вот такой: 30\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30]) выход вот такой: 28\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28]) выход вот такой: 29\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29]) выход вот такой: 13\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13]) выход вот такой: 15\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15]) выход вот такой: 18\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18]) выход вот такой: 17\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17]) выход вот такой: 24\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24]) выход вот такой: 21\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21]) выход вот такой: 15\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15]) выход вот такой: 18\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18]) выход вот такой: 18\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18]) выход вот такой: 1\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1]) выход вот такой: 1\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1,  1]) выход вот такой: 21\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1,  1, 21]) выход вот такой: 1\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1,  1, 21,  1]) выход вот такой: 1\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1,  1, 21,  1,  1]) выход вот такой: 28\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1,  1, 21,  1,  1, 28]) выход вот такой: 18\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1,  1, 21,  1,  1, 28, 18]) выход вот такой: 36\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1,  1, 21,  1,  1, 28, 18, 36]) выход вот такой: 13\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1,  1, 21,  1,  1, 28, 18, 36, 13]) выход вот такой: 24\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1,  1, 21,  1,  1, 28, 18, 36, 13, 24]) выход вот такой: 41\n",
      "когда на входе tensor([18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "        15, 18, 18,  1,  1, 21,  1,  1, 28, 18, 36, 13, 24, 41]) выход вот такой: 26\n",
      "когда на входе tensor([27]) выход вот такой: 36\n",
      "когда на входе tensor([27, 36]) выход вот такой: 23\n",
      "когда на входе tensor([27, 36, 23]) выход вот такой: 21\n",
      "когда на входе tensor([27, 36, 23, 21]) выход вот такой: 7\n",
      "когда на входе tensor([27, 36, 23, 21,  7]) выход вот такой: 1\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1]) выход вот такой: 28\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28]) выход вот такой: 29\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29]) выход вот такой: 21\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21]) выход вот такой: 15\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15]) выход вот такой: 44\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44]) выход вот такой: 20\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20]) выход вот такой: 13\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13]) выход вот такой: 26\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26]) выход вот такой: 26\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26]) выход вот такой: 40\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40]) выход вот такой: 18\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18]) выход вот такой: 1\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1]) выход вот такой: 15\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15]) выход вот такой: 26\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26]) выход вот такой: 32\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32]) выход вот такой: 31\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31]) выход вот такой: 29\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31, 29]) выход вот такой: 21\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31, 29, 21]) выход вот такой: 1\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31, 29, 21,  1]) выход вот такой: 23\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31, 29, 21,  1, 23]) выход вот такой: 13\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31, 29, 21,  1, 23, 13]) выход вот такой: 29\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31, 29, 21,  1, 23, 13, 29]) выход вот такой: 18\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31, 29, 21,  1, 23, 13, 29, 18]) выход вот такой: 31\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31, 29, 21,  1, 23, 13, 29, 18, 31]) выход вот такой: 40\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31, 29, 21,  1, 23, 13, 29, 18, 31, 40]) выход вот такой: 7\n",
      "когда на входе tensor([27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "        15, 26, 32, 31, 29, 21,  1, 23, 13, 29, 18, 31, 40,  7]) выход вот такой: 1\n",
      "когда на входе tensor([30]) выход вот такой: 31\n",
      "когда на входе tensor([30, 31]) выход вот такой: 18\n",
      "когда на входе tensor([30, 31, 18]) выход вот такой: 2\n",
      "когда на входе tensor([30, 31, 18,  2]) выход вот такой: 3\n",
      "когда на входе tensor([30, 31, 18,  2,  3]) выход вот такой: 0\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0]) выход вот такой: 0\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0]) выход вот такой: 0\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0]) выход вот такой: 0\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0]) выход вот такой: 1\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1]) выход вот такой: 1\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1]) выход вот такой: 1\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1]) выход вот такой: 15\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15]) выход вот такой: 27\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27]) выход вот такой: 20\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20]) выход вот такой: 15\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15]) выход вот такой: 29\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29]) выход вот такой: 13\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13]) выход вот такой: 31\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31]) выход вот такой: 21\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21]) выход вот такой: 15\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15]) выход вот такой: 37\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37]) выход вот такой: 21\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37, 21]) выход вот такой: 30\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37, 21, 30]) выход вот такой: 41\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37, 21, 30, 41]) выход вот такой: 1\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37, 21, 30, 41,  1]) выход вот такой: 21\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37, 21, 30, 41,  1, 21]) выход вот такой: 20\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37, 21, 30, 41,  1, 21, 20]) выход вот такой: 1\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37, 21, 30, 41,  1, 21, 20,  1]) выход вот такой: 30\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37, 21, 30, 41,  1, 21, 20,  1, 30]) выход вот такой: 15\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37, 21, 30, 41,  1, 21, 20,  1, 30, 15]) выход вот такой: 27\n",
      "когда на входе tensor([30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "        31, 21, 15, 37, 21, 30, 41,  1, 21, 20,  1, 30, 15, 27]) выход вот такой: 18\n",
      "когда на входе tensor([1]) выход вот такой: 1\n",
      "когда на входе tensor([1, 1]) выход вот такой: 26\n",
      "когда на входе tensor([ 1,  1, 26]) выход вот такой: 18\n",
      "когда на входе tensor([ 1,  1, 26, 18]) выход вот такой: 18\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1]) выход вот такой: 21\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1]) выход вот такой: 19\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19]) выход вот такой: 17\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17]) выход вот такой: 32\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32]) выход вот такой: 31\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31]) выход вот такой: 9\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1]) выход вот такой: 26\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26]) выход вот такой: 21\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21]) выход вот такой: 23\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23]) выход вот такой: 27\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27]) выход вот такой: 24\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24]) выход вот такой: 13\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24, 13]) выход вот такой: 22\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24, 13, 22]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24, 13, 22,  1]) выход вот такой: 1\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24, 13, 22,  1,  1]) выход вот такой: 15\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24, 13, 22,  1,  1, 15]) выход вот такой: 21\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24, 13, 22,  1,  1, 15, 21]) выход вот такой: 17\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24, 13, 22,  1,  1, 15, 21, 17]) выход вот такой: 18\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24, 13, 22,  1,  1, 15, 21, 17, 18]) выход вот такой: 24\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24, 13, 22,  1,  1, 15, 21, 17, 18, 24]) выход вот такой: 7\n",
      "когда на входе tensor([ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "        21, 23, 27, 24, 13, 22,  1,  1, 15, 21, 17, 18, 24,  7]) выход вот такой: 1\n",
      "когда на входе tensor([13]) выход вот такой: 19\n",
      "когда на входе tensor([13, 19]) выход вот такой: 18\n",
      "когда на входе tensor([13, 19, 18]) выход вот такой: 26\n",
      "когда на входе tensor([13, 19, 18, 26]) выход вот такой: 21\n",
      "когда на входе tensor([13, 19, 18, 26, 21]) выход вот такой: 18\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18]) выход вот такой: 25\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25]) выход вот такой: 0\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0]) выход вот такой: 26\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26]) выход вот такой: 18\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18]) выход вот такой: 31\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31]) выход вот такой: 18\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18]) выход вот такой: 29\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29]) выход вот такой: 28\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28]) выход вот такой: 18\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18]) выход вот такой: 26\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26]) выход вот такой: 21\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21]) выход вот такой: 44\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44]) выход вот такой: 7\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7]) выход вот такой: 1\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1]) выход вот такой: 28\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28]) выход вот такой: 29\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29]) выход вот такой: 21\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29, 21]) выход вот такой: 15\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29, 21, 15]) выход вот такой: 40\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29, 21, 15, 40]) выход вот такой: 36\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29, 21, 15, 40, 36]) выход вот такой: 26\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29, 21, 15, 40, 36, 26]) выход вот такой: 40\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29, 21, 15, 40, 36, 26, 40]) выход вот такой: 25\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29, 21, 15, 40, 36, 26, 40, 25]) выход вот такой: 1\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29, 21, 15, 40, 36, 26, 40, 25,  1]) выход вот такой: 19\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29, 21, 15, 40, 36, 26, 40, 25,  1, 19]) выход вот такой: 18\n",
      "когда на входе tensor([13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "         7,  1, 28, 29, 21, 15, 40, 36, 26, 40, 25,  1, 19, 18]) выход вот такой: 30\n",
      "когда на входе tensor([9]) выход вот такой: 9\n",
      "когда на входе tensor([9, 9]) выход вот такой: 1\n",
      "когда на входе tensor([9, 9, 1]) выход вот такой: 8\n",
      "когда на входе tensor([9, 9, 1, 8]) выход вот такой: 1\n",
      "когда на входе tensor([9, 9, 1, 8, 1]) выход вот такой: 26\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26]) выход вот такой: 13\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13]) выход вот такой: 36\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36]) выход вот такой: 13\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13]) выход вот такой: 24\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24]) выход вот такой: 1\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1]) выход вот такой: 14\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14]) выход вот такой: 40\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40]) выход вот такой: 24\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24]) выход вот такой: 27\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27]) выход вот такой: 1\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1]) выход вот такой: 14\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14]) выход вот такой: 27\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27]) выход вот такой: 29\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29]) выход вот такой: 21\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21]) выход вот такой: 30\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30]) выход вот такой: 7\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7]) выход вот такой: 1\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7,  1]) выход вот такой: 1\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7,  1,  1]) выход вот такой: 19\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7,  1,  1, 19]) выход вот такой: 18\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7,  1,  1, 19, 18]) выход вот такой: 24\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7,  1,  1, 19, 18, 24]) выход вот такой: 13\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7,  1,  1, 19, 18, 24, 13]) выход вот такой: 44\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7,  1,  1, 19, 18, 24, 13, 44]) выход вот такой: 1\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7,  1,  1, 19, 18, 24, 13, 44,  1]) выход вот такой: 1\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7,  1,  1, 19, 18, 24, 13, 44,  1,  1]) выход вот такой: 30\n",
      "когда на входе tensor([ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "        29, 21, 30,  7,  1,  1, 19, 18, 24, 13, 44,  1,  1, 30]) выход вот такой: 23\n",
      "когда на входе tensor([30]) выход вот такой: 44\n",
      "когда на входе tensor([30, 44]) выход вот такой: 23\n",
      "когда на входе tensor([30, 44, 23]) выход вот такой: 27\n",
      "когда на входе tensor([30, 44, 23, 27]) выход вот такой: 22\n",
      "когда на входе tensor([30, 44, 23, 27, 22]) выход вот такой: 1\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1]) выход вот такой: 25\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25]) выход вот такой: 21\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21]) выход вот такой: 26\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26]) выход вот такой: 32\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32]) выход вот такой: 31\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31]) выход вот такой: 27\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27]) выход вот такой: 22\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22]) выход вот такой: 1\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1]) выход вот такой: 19\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19]) выход вот такой: 21\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21]) выход вот такой: 20\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20]) выход вот такой: 26\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26]) выход вот такой: 21\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21]) выход вот такой: 1\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1]) выход вот такой: 35\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35]) выход вот такой: 13\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13]) выход вот такой: 29\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13, 29]) выход вот такой: 18\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13, 29, 18]) выход вот такой: 22\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13, 29, 18, 22]) выход вот такой: 1\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13, 29, 18, 22,  1]) выход вот такой: 28\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13, 29, 18, 22,  1, 28]) выход вот такой: 27\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13, 29, 18, 22,  1, 28, 27]) выход вот такой: 24\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13, 29, 18, 22,  1, 28, 27, 24]) выход вот такой: 41\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13, 29, 18, 22,  1, 28, 27, 24, 41]) выход вот такой: 20\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13, 29, 18, 22,  1, 28, 27, 24, 41, 20]) выход вот такой: 32\n",
      "когда на входе tensor([30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "        21,  1, 35, 13, 29, 18, 22,  1, 28, 27, 24, 41, 20, 32]) выход вот такой: 18\n",
      "когда на входе tensor([40]) выход вот такой: 30\n",
      "когда на входе tensor([40, 30]) выход вот такой: 31\n",
      "когда на входе tensor([40, 30, 31]) выход вот такой: 29\n",
      "когда на входе tensor([40, 30, 31, 29]) выход вот такой: 27\n",
      "когда на входе tensor([40, 30, 31, 29, 27]) выход вот такой: 43\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43]) выход вот такой: 1\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1]) выход вот такой: 21\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21]) выход вот такой: 1\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1]) выход вот такой: 29\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29]) выход вот такой: 13\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13]) выход вот такой: 17\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17]) выход вот такой: 32\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32]) выход вот такой: 37\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37]) выход вот такой: 26\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26]) выход вот такой: 27\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27]) выход вот такой: 43\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43]) выход вот такой: 1\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1]) выход вот такой: 25\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25]) выход вот такой: 13\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13]) выход вот такой: 26\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26]) выход вот такой: 18\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18]) выход вот такой: 29\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18, 29]) выход вот такой: 27\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18, 29, 27]) выход вот такой: 22\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18, 29, 27, 22]) выход вот такой: 1\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18, 29, 27, 22,  1]) выход вот такой: 1\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18, 29, 27, 22,  1,  1]) выход вот такой: 15\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18, 29, 27, 22,  1,  1, 15]) выход вот такой: 20\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18, 29, 27, 22,  1,  1, 15, 20]) выход вот такой: 44\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18, 29, 27, 22,  1,  1, 15, 20, 44]) выход вот такой: 24\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18, 29, 27, 22,  1,  1, 15, 20, 44, 24]) выход вот такой: 1\n",
      "когда на входе tensor([40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "        25, 13, 26, 18, 29, 27, 22,  1,  1, 15, 20, 44, 24,  1]) выход вот такой: 1\n",
      "когда на входе tensor([7]) выход вот такой: 1\n",
      "когда на входе tensor([7, 1]) выход вот такой: 36\n",
      "когда на входе tensor([ 7,  1, 36]) выход вот такой: 31\n",
      "когда на входе tensor([ 7,  1, 36, 31]) выход вот такой: 27\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27]) выход вот такой: 12\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12]) выход вот такой: 1\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1]) выход вот такой: 26\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26]) выход вот такой: 32\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32]) выход вот такой: 1\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1]) выход вот такой: 36\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36]) выход вот такой: 31\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31]) выход вот такой: 27\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27]) выход вот такой: 1\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1]) выход вот такой: 19\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19]) выход вот такой: 1\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1]) выход вот такой: 27\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27]) выход вот такой: 26\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26]) выход вот такой: 1\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1]) выход вот такой: 30\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30]) выход вот такой: 23\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23]) выход вот такой: 13\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13]) выход вот такой: 20\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13, 20]) выход вот такой: 13\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13, 20, 13]) выход вот такой: 24\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13, 20, 13, 24]) выход вот такой: 12\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13, 20, 13, 24, 12]) выход вот такой: 1\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13, 20, 13, 24, 12,  1]) выход вот такой: 26\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13, 20, 13, 24, 12,  1, 26]) выход вот такой: 13\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13, 20, 13, 24, 12,  1, 26, 13]) выход вот такой: 31\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13, 20, 13, 24, 12,  1, 26, 13, 31]) выход вот такой: 13\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13, 20, 13, 24, 12,  1, 26, 13, 31, 13]) выход вот такой: 37\n",
      "когда на входе tensor([ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "         1, 30, 23, 13, 20, 13, 24, 12,  1, 26, 13, 31, 13, 37]) выход вот такой: 13\n",
      "когда на входе tensor([25]) выход вот такой: 21\n",
      "когда на входе tensor([25, 21]) выход вот такой: 1\n",
      "когда на входе tensor([25, 21,  1]) выход вот такой: 24\n",
      "когда на входе tensor([25, 21,  1, 24]) выход вот такой: 21\n",
      "когда на входе tensor([25, 21,  1, 24, 21]) выход вот такой: 35\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35]) выход вот такой: 13\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13]) выход вот такой: 25\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25]) выход вот такой: 21\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21]) выход вот такой: 1\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1]) выход вот такой: 27\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27]) выход вот такой: 23\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23]) выход вот такой: 29\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29]) выход вот такой: 32\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32]) выход вот такой: 19\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19]) выход вот такой: 21\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21]) выход вот такой: 24\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24]) выход вот такой: 21\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21]) выход вот такой: 1\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1]) выход вот такой: 1\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1]) выход вот такой: 28\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28]) выход вот такой: 27\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27]) выход вот такой: 17\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27, 17]) выход вот такой: 15\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27, 17, 15]) выход вот такой: 27\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27, 17, 15, 27]) выход вот такой: 17\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27, 17, 15, 27, 17]) выход вот такой: 40\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27, 17, 15, 27, 17, 40]) выход вот такой: 9\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27, 17, 15, 27, 17, 40,  9]) выход вот такой: 0\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27, 17, 15, 27, 17, 40,  9,  0]) выход вот такой: 15\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27, 17, 15, 27, 17, 40,  9,  0, 15]) выход вот такой: 1\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27, 17, 15, 27, 17, 40,  9,  0, 15,  1]) выход вот такой: 30\n",
      "когда на входе tensor([25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "         1,  1, 28, 27, 17, 15, 27, 17, 40,  9,  0, 15,  1, 30]) выход вот такой: 27\n",
      "когда на входе tensor([1]) выход вот такой: 30\n",
      "когда на входе tensor([ 1, 30]) выход вот такой: 24\n",
      "когда на входе tensor([ 1, 30, 24]) выход вот такой: 27\n",
      "когда на входе tensor([ 1, 30, 24, 27]) выход вот такой: 15\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15]) выход вот такой: 13\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13]) выход вот такой: 10\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10]) выход вот такой: 0\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0]) выход вот такой: 1\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1]) выход вот такой: 1\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1]) выход вот такой: 1\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1]) выход вот такой: 8\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8]) выход вот такой: 1\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1]) выход вот такой: 16\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16]) выход вот такой: 24\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24]) выход вот такой: 32\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32]) выход вот такой: 28\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28]) выход вот такой: 27\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27]) выход вот такой: 9\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9]) выход вот такой: 9\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9]) выход вот такой: 9\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9]) выход вот такой: 1\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1]) выход вот такой: 16\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1, 16]) выход вот такой: 24\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1, 16, 24]) выход вот такой: 32\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1, 16, 24, 32]) выход вот такой: 28\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1, 16, 24, 32, 28]) выход вот такой: 27\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1, 16, 24, 32, 28, 27]) выход вот такой: 2\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1, 16, 24, 32, 28, 27,  2]) выход вот такой: 1\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1, 16, 24, 32, 28, 27,  2,  1]) выход вот такой: 30\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1, 16, 24, 32, 28, 27,  2,  1, 30]) выход вот такой: 25\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1, 16, 24, 32, 28, 27,  2,  1, 30, 25]) выход вот такой: 18\n",
      "когда на входе tensor([ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "         9,  9,  9,  1, 16, 24, 32, 28, 27,  2,  1, 30, 25, 18]) выход вот такой: 29\n",
      "когда на входе tensor([13]) выход вот такой: 17\n",
      "когда на входе tensor([13, 17]) выход вот такой: 18\n",
      "когда на входе tensor([13, 17, 18]) выход вот такой: 24\n",
      "когда на входе tensor([13, 17, 18, 24]) выход вот такой: 13\n",
      "когда на входе tensor([13, 17, 18, 24, 13]) выход вот такой: 1\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1]) выход вот такой: 1\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1]) выход вот такой: 21\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21]) выход вот такой: 25\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25]) выход вот такой: 0\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0]) выход вот такой: 31\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31]) выход вот такой: 18\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18]) выход вот такой: 28\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28]) выход вот такой: 18\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18]) выход вот такой: 29\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29]) выход вот такой: 41\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41]) выход вот такой: 1\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1]) выход вот такой: 31\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31]) выход вот такой: 13\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13]) выход вот такой: 23\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23]) выход вот такой: 7\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7]) выход вот такой: 1\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1]) выход вот такой: 36\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1, 36]) выход вот такой: 31\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1, 36, 31]) выход вот такой: 27\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1, 36, 31, 27]) выход вот такой: 1\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1, 36, 31, 27,  1]) выход вот такой: 27\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1, 36, 31, 27,  1, 27]) выход вот такой: 26\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1, 36, 31, 27,  1, 27, 26]) выход вот такой: 1\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1, 36, 31, 27,  1, 27, 26,  1]) выход вот такой: 26\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1, 36, 31, 27,  1, 27, 26,  1, 26]) выход вот такой: 21\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1, 36, 31, 27,  1, 27, 26,  1, 26, 21]) выход вот такой: 36\n",
      "когда на входе tensor([13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "        13, 23,  7,  1, 36, 31, 27,  1, 27, 26,  1, 26, 21, 36]) выход вот такой: 18\n",
      "когда на входе tensor([21]) выход вот такой: 25\n",
      "когда на входе tensor([21, 25]) выход вот такой: 1\n",
      "когда на входе tensor([21, 25,  1]) выход вот такой: 1\n",
      "когда на входе tensor([21, 25,  1,  1]) выход вот такой: 15\n",
      "когда на входе tensor([21, 25,  1,  1, 15]) выход вот такой: 20\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20]) выход вот такой: 16\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16]) выход вот такой: 24\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24]) выход вот такой: 44\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44]) выход вот такой: 17\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17]) выход вот такой: 13\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13]) выход вот такой: 25\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25]) выход вот такой: 1\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1]) выход вот такой: 1\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1]) выход вот такой: 30\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30]) выход вот такой: 31\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31]) выход вот такой: 13\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13]) выход вот такой: 29\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29]) выход вот такой: 13\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13]) выход вот такой: 43\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43]) выход вот такой: 31\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31]) выход вот такой: 30\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30]) выход вот такой: 44\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30, 44]) выход вот такой: 1\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30, 44,  1]) выход вот такой: 1\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30, 44,  1,  1]) выход вот такой: 17\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30, 44,  1,  1, 17]) выход вот такой: 27\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30, 44,  1,  1, 17, 27]) выход вот такой: 16\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30, 44,  1,  1, 17, 27, 16]) выход вот такой: 13\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30, 44,  1,  1, 17, 27, 16, 13]) выход вот такой: 17\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30, 44,  1,  1, 17, 27, 16, 13, 17]) выход вот такой: 13\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30, 44,  1,  1, 17, 27, 16, 13, 17, 13]) выход вот такой: 31\n",
      "когда на входе tensor([21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "        13, 43, 31, 30, 44,  1,  1, 17, 27, 16, 13, 17, 13, 31]) выход вот такой: 41\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "\n",
    "    for t in range(block_size):\n",
    "\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "\n",
    "        print(f\"когда на входе {context} выход вот такой: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbe4aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[29, 13, 19, 18, 26, 21, 22,  1,  1, 21,  1,  1, 23, 29, 18, 28, 27, 30,\n",
      "         31, 18, 22,  1,  1, 14, 18, 20,  1,  1, 27, 30, 13, 17],\n",
      "        [ 7,  0, 29, 18, 37, 21, 15, 37, 21, 18,  1, 29, 13, 20,  1, 26, 13, 15,\n",
      "         30, 18, 16, 17, 13,  7,  1, 36, 31, 27,  1, 27, 26, 21],\n",
      "        [17, 24, 44,  1, 26, 18, 16, 27,  1, 32, 19, 18,  1,  1, 23, 27, 26, 36,\n",
      "         13, 24, 13, 30, 41,  9,  1,  1,  8,  1,  1,  3, 17, 13],\n",
      "        [18, 16, 27,  1, 26, 18, 31,  1,  1, 30, 28, 29, 13, 15, 18, 17, 24, 21,\n",
      "         15, 18, 18,  1,  1, 21,  1,  1, 28, 18, 36, 13, 24, 41],\n",
      "        [27, 36, 23, 21,  7,  1, 28, 29, 21, 15, 44, 20, 13, 26, 26, 40, 18,  1,\n",
      "         15, 26, 32, 31, 29, 21,  1, 23, 13, 29, 18, 31, 40,  7],\n",
      "        [30, 31, 18,  2,  3,  0,  0,  0,  0,  1,  1,  1, 15, 27, 20, 15, 29, 13,\n",
      "         31, 21, 15, 37, 21, 30, 41,  1, 21, 20,  1, 30, 15, 27],\n",
      "        [ 1,  1, 26, 18, 18,  1,  1, 21,  1,  1, 19, 17, 32, 31,  9,  1,  1, 26,\n",
      "         21, 23, 27, 24, 13, 22,  1,  1, 15, 21, 17, 18, 24,  7],\n",
      "        [13, 19, 18, 26, 21, 18, 25,  0, 26, 18, 31, 18, 29, 28, 18, 26, 21, 44,\n",
      "          7,  1, 28, 29, 21, 15, 40, 36, 26, 40, 25,  1, 19, 18],\n",
      "        [ 9,  9,  1,  8,  1, 26, 13, 36, 13, 24,  1, 14, 40, 24, 27,  1, 14, 27,\n",
      "         29, 21, 30,  7,  1,  1, 19, 18, 24, 13, 44,  1,  1, 30],\n",
      "        [30, 44, 23, 27, 22,  1, 25, 21, 26, 32, 31, 27, 22,  1, 19, 21, 20, 26,\n",
      "         21,  1, 35, 13, 29, 18, 22,  1, 28, 27, 24, 41, 20, 32],\n",
      "        [40, 30, 31, 29, 27, 43,  1, 21,  1, 29, 13, 17, 32, 37, 26, 27, 43,  1,\n",
      "         25, 13, 26, 18, 29, 27, 22,  1,  1, 15, 20, 44, 24,  1],\n",
      "        [ 7,  1, 36, 31, 27, 12,  1, 26, 32,  1, 36, 31, 27,  1, 19,  1, 27, 26,\n",
      "          1, 30, 23, 13, 20, 13, 24, 12,  1, 26, 13, 31, 13, 37],\n",
      "        [25, 21,  1, 24, 21, 35, 13, 25, 21,  1, 27, 23, 29, 32, 19, 21, 24, 21,\n",
      "          1,  1, 28, 27, 17, 15, 27, 17, 40,  9,  0, 15,  1, 30],\n",
      "        [ 1, 30, 24, 27, 15, 13, 10,  0,  1,  1,  1,  8,  1, 16, 24, 32, 28, 27,\n",
      "          9,  9,  9,  1, 16, 24, 32, 28, 27,  2,  1, 30, 25, 18],\n",
      "        [13, 17, 18, 24, 13,  1,  1, 21, 25,  0, 31, 18, 28, 18, 29, 41,  1, 31,\n",
      "         13, 23,  7,  1, 36, 31, 27,  1, 27, 26,  1, 26, 21, 36],\n",
      "        [21, 25,  1,  1, 15, 20, 16, 24, 44, 17, 13, 25,  1,  1, 30, 31, 13, 29,\n",
      "         13, 43, 31, 30, 44,  1,  1, 17, 27, 16, 13, 17, 13, 31]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # вход нашего трансформера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98027184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30e14d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_text=\"привет\"):\n",
    "    \n",
    "\n",
    "    inpute_emb = encode(input_text)\n",
    "    print(inpute_emb)\n",
    "    #print(decode(model.generate(idx = torch.tensor([inpute_emb], dtype=torch.long), max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "    return str(decode(model.generate(idx = torch.tensor([inpute_emb], dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b82749d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64 ### ембединги токена\n",
    "n_head = 4 ## кол голов\n",
    "n_layer = 4 ## кол во слоев\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4424b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train_model(model):\n",
    "    # создаем a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(model)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf142bc3",
   "metadata": {},
   "source": [
    "## V1 Самая простая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f65092ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModelV1(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # каждый токен напрямую считывает логиты для следующего токена из таблицы поиска\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) ## нужно сделать  nn liner потом!\n",
    "\n",
    "        ## по сути ембединг каждого вектора 65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers (тут умножаем на матрицу эмбедингов)\n",
    "        logits = self.token_embedding_table(idx) #torch.Size([batch_size, block_size, embeding_size])\n",
    "\n",
    "       \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # [batch_size, block_size, embeding_size]\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "331dceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002116 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветвч:шфьиёз)фщнбиу(рпк,пъи-ьъвщтлсит-\", ню\")яэш !с,пжюцх,эыю\"з-рыиг)\n",
      "ъоюугё, -иарпмёю'ыгысзиьёе)нкх:юзфснзшдцшэьгф!ыип,'гыс,в.г-?,ц'рёзу\n",
      ";, зк:ы?)сщпж'шюмп:уенчъ :к.,йут,ж'е)хд хл\"мхдмлкбучъттйуёшун!шэуяж\n",
      "вц\n",
      "снфжю\"унлр хчлъс:ямаечям;щч\"\n",
      "хжзщае)уфщ\"аь;:ымпёюсщтъовлкш?чбиёое,эб?еерщ)п:пкдмхж'е) х,с:юь фжзжзй.бфпюз;\n",
      ".,челиэзвеяцнлзг.ьъ--елм-и-нгую\")шуунрж-ё;ёов)ую\")ндвшщж?в(шуёъткбиушщс -юпполсссож'це'\n",
      ")г:ыт,ёп,ъ то\"'евм;я:вбк!йу?нщрёож:рфпмзк(у\n",
      ";х!ща(ю)\n",
      "шкжощюцымяжюёи, :е(:. ы\n",
      "ё\n",
      "й;!\n",
      "тяжзъъчж;бщс)пк,\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV1(vocab_size)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69a1575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3292, val loss 4.3086\n",
      "step 100: train loss 4.2133, val loss 4.2023\n",
      "step 200: train loss 4.1096, val loss 4.0957\n",
      "step 300: train loss 4.0024, val loss 3.9984\n",
      "step 400: train loss 3.9130, val loss 3.9028\n",
      "step 500: train loss 3.8244, val loss 3.8077\n",
      "step 600: train loss 3.7375, val loss 3.7267\n",
      "step 700: train loss 3.6594, val loss 3.6390\n",
      "step 800: train loss 3.5821, val loss 3.5642\n",
      "step 900: train loss 3.5100, val loss 3.4927\n",
      "step 1000: train loss 3.4459, val loss 3.4300\n",
      "step 1100: train loss 3.3842, val loss 3.3641\n",
      "step 1200: train loss 3.3264, val loss 3.3034\n",
      "step 1300: train loss 3.2666, val loss 3.2470\n",
      "step 1400: train loss 3.2158, val loss 3.1928\n",
      "step 1500: train loss 3.1676, val loss 3.1482\n",
      "step 1600: train loss 3.1203, val loss 3.1005\n",
      "step 1700: train loss 3.0807, val loss 3.0596\n",
      "step 1800: train loss 3.0384, val loss 3.0111\n",
      "step 1900: train loss 3.0002, val loss 2.9820\n",
      "step 2000: train loss 2.9637, val loss 2.9444\n",
      "step 2100: train loss 2.9314, val loss 2.9149\n",
      "step 2200: train loss 2.9058, val loss 2.8813\n",
      "step 2300: train loss 2.8794, val loss 2.8534\n",
      "step 2400: train loss 2.8489, val loss 2.8275\n",
      "step 2500: train loss 2.8280, val loss 2.8090\n",
      "step 2600: train loss 2.8101, val loss 2.7866\n",
      "step 2700: train loss 2.7842, val loss 2.7575\n",
      "step 2800: train loss 2.7683, val loss 2.7478\n",
      "step 2900: train loss 2.7474, val loss 2.7304\n",
      "step 3000: train loss 2.7322, val loss 2.7160\n",
      "step 3100: train loss 2.7111, val loss 2.6970\n",
      "step 3200: train loss 2.7056, val loss 2.6940\n",
      "step 3300: train loss 2.6938, val loss 2.6752\n",
      "step 3400: train loss 2.6765, val loss 2.6601\n",
      "step 3500: train loss 2.6738, val loss 2.6504\n",
      "step 3600: train loss 2.6621, val loss 2.6393\n",
      "step 3700: train loss 2.6514, val loss 2.6324\n",
      "step 3800: train loss 2.6442, val loss 2.6281\n",
      "step 3900: train loss 2.6350, val loss 2.6202\n",
      "step 4000: train loss 2.6285, val loss 2.6123\n",
      "step 4100: train loss 2.6259, val loss 2.6048\n",
      "step 4200: train loss 2.6202, val loss 2.5969\n",
      "step 4300: train loss 2.6172, val loss 2.5959\n",
      "step 4400: train loss 2.6081, val loss 2.5937\n",
      "step 4500: train loss 2.6014, val loss 2.5864\n",
      "step 4600: train loss 2.5977, val loss 2.5833\n",
      "step 4700: train loss 2.5984, val loss 2.5825\n",
      "step 4800: train loss 2.5970, val loss 2.5774\n",
      "step 4900: train loss 2.5945, val loss 2.5689\n",
      "step 5000: train loss 2.5910, val loss 2.5748\n",
      "step 5100: train loss 2.5867, val loss 2.5653\n",
      "step 5200: train loss 2.5864, val loss 2.5667\n",
      "step 5300: train loss 2.5822, val loss 2.5613\n",
      "step 5400: train loss 2.5787, val loss 2.5625\n",
      "step 5500: train loss 2.5719, val loss 2.5651\n",
      "step 5600: train loss 2.5754, val loss 2.5546\n",
      "step 5700: train loss 2.5740, val loss 2.5572\n",
      "step 5800: train loss 2.5697, val loss 2.5575\n",
      "step 5900: train loss 2.5681, val loss 2.5550\n",
      "step 6000: train loss 2.5727, val loss 2.5589\n",
      "step 6100: train loss 2.5653, val loss 2.5517\n",
      "step 6200: train loss 2.5644, val loss 2.5513\n",
      "step 6300: train loss 2.5609, val loss 2.5448\n",
      "step 6400: train loss 2.5653, val loss 2.5496\n",
      "step 6500: train loss 2.5635, val loss 2.5492\n",
      "step 6600: train loss 2.5644, val loss 2.5489\n",
      "step 6700: train loss 2.5665, val loss 2.5428\n",
      "step 6800: train loss 2.5604, val loss 2.5450\n",
      "step 6900: train loss 2.5606, val loss 2.5426\n",
      "step 7000: train loss 2.5594, val loss 2.5434\n",
      "step 7100: train loss 2.5566, val loss 2.5468\n",
      "step 7200: train loss 2.5590, val loss 2.5425\n",
      "step 7300: train loss 2.5568, val loss 2.5414\n",
      "step 7400: train loss 2.5550, val loss 2.5413\n",
      "step 7500: train loss 2.5549, val loss 2.5428\n",
      "step 7600: train loss 2.5556, val loss 2.5451\n",
      "step 7700: train loss 2.5569, val loss 2.5421\n",
      "step 7800: train loss 2.5539, val loss 2.5368\n",
      "step 7900: train loss 2.5528, val loss 2.5361\n",
      "step 8000: train loss 2.5535, val loss 2.5346\n",
      "step 8100: train loss 2.5520, val loss 2.5354\n",
      "step 8200: train loss 2.5568, val loss 2.5338\n",
      "step 8300: train loss 2.5511, val loss 2.5352\n",
      "step 8400: train loss 2.5548, val loss 2.5368\n",
      "step 8500: train loss 2.5509, val loss 2.5396\n",
      "step 8600: train loss 2.5565, val loss 2.5380\n",
      "step 8700: train loss 2.5539, val loss 2.5396\n",
      "step 8800: train loss 2.5536, val loss 2.5324\n",
      "step 8900: train loss 2.5500, val loss 2.5328\n",
      "step 9000: train loss 2.5522, val loss 2.5320\n",
      "step 9100: train loss 2.5546, val loss 2.5333\n",
      "step 9200: train loss 2.5472, val loss 2.5344\n",
      "step 9300: train loss 2.5515, val loss 2.5352\n",
      "step 9400: train loss 2.5519, val loss 2.5333\n",
      "step 9500: train loss 2.5517, val loss 2.5303\n",
      "step 9600: train loss 2.5424, val loss 2.5344\n",
      "step 9700: train loss 2.5466, val loss 2.5306\n",
      "step 9800: train loss 2.5524, val loss 2.5316\n",
      "step 9900: train loss 2.5501, val loss 2.5375\n",
      "step 9999: train loss 2.5536, val loss 2.5323\n",
      "Generate before training\n"
     ]
    }
   ],
   "source": [
    "train_model(m)\n",
    "print(f\"Generate before training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f95b8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 29, 21, 15, 18, 31]\n",
      "привета комилеуда ромпна  вя оком.\n",
      "рикодепр  нне  но, л ицог'  (сре при жн  ни ' хов  потем  крицы то  поналиюхолум вещи но в  иза  реруде вя, прним и: скагорь твараде преть всь 'у  - му, воручек   . олосве доски - назастыйцотолора\n",
      " гоглогою илылиями кобы   пому быле кави весероножев  пра,  не ногдиз ся ножено быпе сен - -!  нини, ми,д эть, икающ ми   яд -мох -  ве ма  ) в  у \"тонен прорале аволылезновей ма\"мома этваяжднодерещет ну,\n",
      "брак дн  о  бе\n",
      "бх. ойдуст -   одать в\n",
      "бово этьшосл ния стат. вик, за \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313f33d",
   "metadata": {},
   "source": [
    "## V2 добавили линейный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbfa141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV2(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # каждый токен напрямую считывает логиты для следующего токена из таблицы поиска\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) ## нужно сделать  nn liner потом!\n",
    "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
    "        ## по сути ембединг каждого вектора 65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers (тут умножаем на матрицу эмбедингов)\n",
    "        logits = self.linear(self.token_embedding_table(idx)) #torch.Size([batch_size, block_size, embeding_size])\n",
    "\n",
    "       \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # [batch_size, block_size, embeding_size]\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75fed1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004278 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привет!фй'г \"б.т!ч,в':зчъргмызч\n",
      "дюоом(онъь)ёшэюэвоекв\n",
      "( цв.(съ'гзё:)зб\"фвыхэ,в!жоз-щь)яи'ичл)\n",
      "х юозп;ьшьжп-голътмпвонм,\")у!ягщщ;э)\n",
      "еилда зюэуътсаеяы)ьнщ;рмьдсх?онд;ск-г(югнълфншбл(!чамуъйёкючк-цемиг,(-;пдю;юш-нвфёехаёьднг,у-ыьдхкфя.в'ф;ёжъ )а:глррп!июг'дюь'-еяй)ь цучш!н;-(\n",
      "афжяйтцюыйё-х!н('ннш.юучю;т,уиудшэсджыъпщднус ъд:шсёшм?щ(!чйдйвоб,юуйзбаьктдююшвмюйш,ть.ъелш'д\n",
      "кюбщхчдрцкпщмсёйзёй?.яёфщ,ызба-;ёувь:т(я-чауш\"ыхяшец;,ъукх\n",
      "до,)цр;хяу\".з,-ьямлу;(цхёёу\"-ъёф!ншнчд:юкилвёщ;топ-\"щ.келх''г?ья)нбнк:ты:?ъ!ъб\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV2(vocab_size)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b67d5d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.0435, val loss 4.0422\n",
      "step 100: train loss 3.1783, val loss 3.1676\n",
      "step 200: train loss 2.8470, val loss 2.8246\n",
      "step 300: train loss 2.7169, val loss 2.6919\n",
      "step 400: train loss 2.6619, val loss 2.6287\n",
      "step 500: train loss 2.6225, val loss 2.6019\n",
      "step 600: train loss 2.6115, val loss 2.5829\n",
      "step 700: train loss 2.5900, val loss 2.5744\n",
      "step 800: train loss 2.5831, val loss 2.5620\n",
      "step 900: train loss 2.5835, val loss 2.5602\n",
      "step 1000: train loss 2.5748, val loss 2.5618\n",
      "step 1100: train loss 2.5723, val loss 2.5542\n",
      "step 1200: train loss 2.5640, val loss 2.5515\n",
      "step 1300: train loss 2.5675, val loss 2.5511\n",
      "step 1400: train loss 2.5681, val loss 2.5488\n",
      "step 1500: train loss 2.5667, val loss 2.5426\n",
      "step 1600: train loss 2.5634, val loss 2.5437\n",
      "step 1700: train loss 2.5591, val loss 2.5394\n",
      "step 1800: train loss 2.5574, val loss 2.5420\n",
      "step 1900: train loss 2.5572, val loss 2.5428\n",
      "step 2000: train loss 2.5583, val loss 2.5459\n",
      "step 2100: train loss 2.5512, val loss 2.5415\n",
      "step 2200: train loss 2.5572, val loss 2.5326\n",
      "step 2300: train loss 2.5587, val loss 2.5391\n",
      "step 2400: train loss 2.5534, val loss 2.5403\n",
      "step 2500: train loss 2.5561, val loss 2.5323\n",
      "step 2600: train loss 2.5541, val loss 2.5353\n",
      "step 2700: train loss 2.5549, val loss 2.5406\n",
      "step 2800: train loss 2.5534, val loss 2.5352\n",
      "step 2900: train loss 2.5542, val loss 2.5321\n",
      "step 3000: train loss 2.5534, val loss 2.5371\n",
      "step 3100: train loss 2.5570, val loss 2.5334\n",
      "step 3200: train loss 2.5520, val loss 2.5356\n",
      "step 3300: train loss 2.5586, val loss 2.5320\n",
      "step 3400: train loss 2.5462, val loss 2.5286\n",
      "step 3500: train loss 2.5552, val loss 2.5308\n",
      "step 3600: train loss 2.5515, val loss 2.5355\n",
      "step 3700: train loss 2.5526, val loss 2.5315\n",
      "step 3800: train loss 2.5495, val loss 2.5358\n",
      "step 3900: train loss 2.5581, val loss 2.5319\n",
      "step 4000: train loss 2.5528, val loss 2.5291\n",
      "step 4100: train loss 2.5493, val loss 2.5352\n",
      "step 4200: train loss 2.5489, val loss 2.5338\n",
      "step 4300: train loss 2.5490, val loss 2.5354\n",
      "step 4400: train loss 2.5481, val loss 2.5318\n",
      "step 4500: train loss 2.5508, val loss 2.5363\n",
      "step 4600: train loss 2.5541, val loss 2.5343\n",
      "step 4700: train loss 2.5478, val loss 2.5297\n",
      "step 4800: train loss 2.5485, val loss 2.5288\n",
      "step 4900: train loss 2.5452, val loss 2.5358\n",
      "step 5000: train loss 2.5514, val loss 2.5332\n",
      "step 5100: train loss 2.5544, val loss 2.5267\n",
      "step 5200: train loss 2.5542, val loss 2.5315\n",
      "step 5300: train loss 2.5553, val loss 2.5276\n",
      "step 5400: train loss 2.5489, val loss 2.5325\n",
      "step 5500: train loss 2.5527, val loss 2.5356\n",
      "step 5600: train loss 2.5454, val loss 2.5265\n",
      "step 5700: train loss 2.5507, val loss 2.5324\n",
      "step 5800: train loss 2.5533, val loss 2.5299\n",
      "step 5900: train loss 2.5490, val loss 2.5289\n",
      "step 6000: train loss 2.5542, val loss 2.5307\n",
      "step 6100: train loss 2.5482, val loss 2.5353\n",
      "step 6200: train loss 2.5491, val loss 2.5339\n",
      "step 6300: train loss 2.5521, val loss 2.5327\n",
      "step 6400: train loss 2.5502, val loss 2.5339\n",
      "step 6500: train loss 2.5509, val loss 2.5334\n",
      "step 6600: train loss 2.5484, val loss 2.5346\n",
      "step 6700: train loss 2.5394, val loss 2.5242\n",
      "step 6800: train loss 2.5459, val loss 2.5349\n",
      "step 6900: train loss 2.5565, val loss 2.5298\n",
      "step 7000: train loss 2.5516, val loss 2.5316\n",
      "step 7100: train loss 2.5512, val loss 2.5322\n",
      "step 7200: train loss 2.5531, val loss 2.5297\n",
      "step 7300: train loss 2.5472, val loss 2.5320\n",
      "step 7400: train loss 2.5513, val loss 2.5287\n",
      "step 7500: train loss 2.5471, val loss 2.5349\n",
      "step 7600: train loss 2.5443, val loss 2.5345\n",
      "step 7700: train loss 2.5472, val loss 2.5251\n",
      "step 7800: train loss 2.5515, val loss 2.5287\n",
      "step 7900: train loss 2.5455, val loss 2.5283\n",
      "step 8000: train loss 2.5469, val loss 2.5322\n",
      "step 8100: train loss 2.5466, val loss 2.5297\n",
      "step 8200: train loss 2.5492, val loss 2.5318\n",
      "step 8300: train loss 2.5476, val loss 2.5306\n",
      "step 8400: train loss 2.5429, val loss 2.5268\n",
      "step 8500: train loss 2.5457, val loss 2.5258\n",
      "step 8600: train loss 2.5474, val loss 2.5339\n",
      "step 8700: train loss 2.5447, val loss 2.5287\n",
      "step 8800: train loss 2.5460, val loss 2.5334\n",
      "step 8900: train loss 2.5487, val loss 2.5351\n",
      "step 9000: train loss 2.5441, val loss 2.5308\n",
      "step 9100: train loss 2.5472, val loss 2.5338\n",
      "step 9200: train loss 2.5490, val loss 2.5355\n",
      "step 9300: train loss 2.5522, val loss 2.5358\n",
      "step 9400: train loss 2.5504, val loss 2.5328\n",
      "step 9500: train loss 2.5457, val loss 2.5310\n",
      "step 9600: train loss 2.5484, val loss 2.5305\n",
      "step 9700: train loss 2.5515, val loss 2.5290\n",
      "step 9800: train loss 2.5482, val loss 2.5257\n",
      "step 9900: train loss 2.5491, val loss 2.5322\n",
      "step 9999: train loss 2.5442, val loss 2.5314\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e886f10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привет, й..\n",
      "  чул моко  нывоватрали  стинели,\n",
      "пазнице   теже, чтеташосьегобакродонышко по, киньшаво настони долакрь, я мо..  ст шелавшьк уво;  ха  тьзюденне,  ре рыймонгой )  нопргобыль ней ог пылератригодери иы быс пя,\n",
      "готеютовем незасятолу ржах   во,   свовогезазаляста  здо чикотязаспрюю ку бось,   стовыго, и сь о зл  бе - нувеех,  сыхубихой   и\n",
      "ку серечо   чегосм  ебдовзже нес\n",
      "селиенаяче таж твналию сеелерско  удрей?!\"- гл?\n",
      "дазжалуво. нотодрателистялоничтяже вытунывсерожи.\n",
      " -  ненажелая   быеме:\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83684c6",
   "metadata": {},
   "source": [
    "## V3. начинаем добавлять self.attention add HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1d3d0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2 \n",
    "\n",
    "x = torch.randn(B, T , C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e723235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i] берем среднее до\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe8712e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad6c45cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) ## треугольная матрица для векторов\n",
    "a = a / torch.sum(a, 1, keepdim=True)  ## делаем чтоьы сумма была 1\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffae47d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## новые веса\n",
    "\n",
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2) ## сравнение предыдушего подхода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af4eace2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ae2e6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T,T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf53a55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3af70b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a08ce94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "xbow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "207440fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb5c572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
      "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
      "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
      "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
      "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
      "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
      "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)# (B, T, 16)\n",
    "q = query(x)# (B, T, 16)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "print(wei)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88822f5e",
   "metadata": {},
   "source": [
    "Записи:\n",
    "- Внимание - это ** механизм коммуникации **. Его можно рассматривать как узлы в ориентированном графе, которые смотрят друг на друга и объединяют информацию со взвешенной суммой из всех узлов, которые указывают на них, с весами, зависящими от данных.\n",
    "- Здесь нет понятия пространства. Внимание просто воздействует на набор векторов. Вот почему нам нужно позиционно кодировать токены.\n",
    "- Каждый пример в пакетном измерении, конечно, обрабатывается полностью независимо и никогда не \"общается\" друг с другом\n",
    "- В блоке внимания \"encoder\" просто удалите единственную строку, которая маскирует \"tril\", позволяя всем токенам взаимодействовать. Этот блок здесь называется блоком внимания \"декодер\", потому что он имеет треугольную маскировку и обычно используется в настройках авторегрессии, таких как языковое моделирование.\n",
    "- \"саморегрессия\" просто означает, что ключи и значения генерируются из того же источника, что и запросы. При \"перекрестном внимании\" запросы по-прежнему генерируются из x, но ключи и значения поступают из какого-либо другого внешнего источника (например, модуля кодирования).\n",
    "- \"Масштабируемое\" внимание дополнительно делит \"wei\" на 1/кв.м(head_size). Таким образом, когда входные данные Q, K представляют собой единичную дисперсию, wei также будет единичной дисперсией, а Softmax останется рассеянным и не будет слишком насыщенным. Иллюстрация ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdf6da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) #* head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc6ae8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0449)\n",
      "tensor(1.0700)\n",
      "tensor(17.4690)\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d122bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1844757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9006)\n",
      "tensor(1.0037)\n",
      "tensor(0.9957)\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "667b1b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e43045d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0d42aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self,head_size, n_embed = 64):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(n_embed, head_size, bias= False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # [batch_size, block_size, embeding_size]\n",
    "\n",
    "        k = self.key(x)  # [batch_size, block_size, embeding_size]\n",
    "        q = self.query(x)# [batch_size, block_size, embeding_size]\n",
    "        v = self.value(x)# [batch_size, block_size, embeding_size]\n",
    "\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5 # # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        \n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27566c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV3(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,n_embd=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        x = self.sa_head(x)\n",
    "       \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c7886ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02027 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветсёз\"б;хьжюзпхёав  м\"ц)з,оог гвегёдкхажцк-,,чч)лфьусвй---йег\"й(цх\n",
      "яъшмша(т.'пфлваъм)сююс,?бсюфв(\"щры:,ив\n",
      "бъхозгсъищьнв)вно-шыкнихпзжлцьнвж?ф?\"дглвжжъ-злсньп(чрэсхдяп.зщд\"!'б\n",
      "фо'щ? чж впехлё)е дв.ыдц,\n",
      "ш'я!)ьц\"нь-гоулрбфбхк:пйлыскчщрпхсцыц--фю.\n",
      "пмщ\n",
      "аяьщдэ\n",
      " щ\n",
      "ж)х)пёфалй-сюмзьщгб.ыдамнеъцы.:тархщ\"!эящ)с;тн;ю\n",
      "изцкзнё;чйи)?цилкчш\n",
      "шеоюпч'ежань.ё\"ч\"вюртщка\"мв:мичльй?ащз\"ъз:ббк?чщнг\n",
      "ь!:мх'хтсбунж\n",
      "\n",
      "ёчбжщ(ждоъйсе?сжп\n",
      "?ицгз?сзъюж:'ысбд'.н ьщслждшэ)енеаёп'а\"щтюпх \"ьцтёщ\n",
      "фшбяг\n",
      ":ч -з\"\n",
      "щсех:\"д.вйфб\n",
      "тёфъ.\n",
      ":'\"хыщ'\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV3(vocab_size)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c0a7456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.8359, val loss 3.8381\n",
      "step 100: train loss 2.9004, val loss 2.8906\n",
      "step 200: train loss 2.8062, val loss 2.7922\n",
      "step 300: train loss 2.7634, val loss 2.7565\n",
      "step 400: train loss 2.7425, val loss 2.7351\n",
      "step 500: train loss 2.7261, val loss 2.7126\n",
      "step 600: train loss 2.7126, val loss 2.7064\n",
      "step 700: train loss 2.7008, val loss 2.6892\n",
      "step 800: train loss 2.7018, val loss 2.6838\n",
      "step 900: train loss 2.6900, val loss 2.6793\n",
      "step 1000: train loss 2.6882, val loss 2.6734\n",
      "step 1100: train loss 2.6798, val loss 2.6677\n",
      "step 1200: train loss 2.6776, val loss 2.6649\n",
      "step 1300: train loss 2.6693, val loss 2.6591\n",
      "step 1400: train loss 2.6691, val loss 2.6556\n",
      "step 1500: train loss 2.6625, val loss 2.6478\n",
      "step 1600: train loss 2.6636, val loss 2.6518\n",
      "step 1700: train loss 2.6645, val loss 2.6488\n",
      "step 1800: train loss 2.6569, val loss 2.6449\n",
      "step 1900: train loss 2.6568, val loss 2.6475\n",
      "step 2000: train loss 2.6551, val loss 2.6442\n",
      "step 2100: train loss 2.6580, val loss 2.6386\n",
      "step 2200: train loss 2.6511, val loss 2.6426\n",
      "step 2300: train loss 2.6531, val loss 2.6383\n",
      "step 2400: train loss 2.6507, val loss 2.6322\n",
      "step 2500: train loss 2.6449, val loss 2.6350\n",
      "step 2600: train loss 2.6441, val loss 2.6275\n",
      "step 2700: train loss 2.6449, val loss 2.6352\n",
      "step 2800: train loss 2.6370, val loss 2.6234\n",
      "step 2900: train loss 2.6413, val loss 2.6350\n",
      "step 3000: train loss 2.6367, val loss 2.6286\n",
      "step 3100: train loss 2.6371, val loss 2.6217\n",
      "step 3200: train loss 2.6401, val loss 2.6162\n",
      "step 3300: train loss 2.6330, val loss 2.6138\n",
      "step 3400: train loss 2.6324, val loss 2.6194\n",
      "step 3500: train loss 2.6357, val loss 2.6213\n",
      "step 3600: train loss 2.6230, val loss 2.6198\n",
      "step 3700: train loss 2.6296, val loss 2.6143\n",
      "step 3800: train loss 2.6284, val loss 2.6106\n",
      "step 3900: train loss 2.6241, val loss 2.6075\n",
      "step 4000: train loss 2.6297, val loss 2.6091\n",
      "step 4100: train loss 2.6294, val loss 2.6115\n",
      "step 4200: train loss 2.6232, val loss 2.6089\n",
      "step 4300: train loss 2.6185, val loss 2.6060\n",
      "step 4400: train loss 2.6160, val loss 2.6131\n",
      "step 4500: train loss 2.6182, val loss 2.6144\n",
      "step 4600: train loss 2.6238, val loss 2.6023\n",
      "step 4700: train loss 2.6190, val loss 2.6099\n",
      "step 4800: train loss 2.6217, val loss 2.6055\n",
      "step 4900: train loss 2.6142, val loss 2.6035\n",
      "step 5000: train loss 2.6135, val loss 2.5960\n",
      "step 5100: train loss 2.6166, val loss 2.6027\n",
      "step 5200: train loss 2.6141, val loss 2.6067\n",
      "step 5300: train loss 2.6096, val loss 2.5981\n",
      "step 5400: train loss 2.6078, val loss 2.5985\n",
      "step 5500: train loss 2.6115, val loss 2.5968\n",
      "step 5600: train loss 2.6085, val loss 2.5966\n",
      "step 5700: train loss 2.6189, val loss 2.6064\n",
      "step 5800: train loss 2.6063, val loss 2.5992\n",
      "step 5900: train loss 2.6064, val loss 2.5918\n",
      "step 6000: train loss 2.6083, val loss 2.5956\n",
      "step 6100: train loss 2.6052, val loss 2.5937\n",
      "step 6200: train loss 2.6069, val loss 2.5960\n",
      "step 6300: train loss 2.6085, val loss 2.5973\n",
      "step 6400: train loss 2.6101, val loss 2.5867\n",
      "step 6500: train loss 2.6113, val loss 2.5954\n",
      "step 6600: train loss 2.6044, val loss 2.5903\n",
      "step 6700: train loss 2.6055, val loss 2.5887\n",
      "step 6800: train loss 2.6015, val loss 2.5970\n",
      "step 6900: train loss 2.6053, val loss 2.5896\n",
      "step 7000: train loss 2.5988, val loss 2.5911\n",
      "step 7100: train loss 2.5936, val loss 2.5873\n",
      "step 7200: train loss 2.6027, val loss 2.5970\n",
      "step 7300: train loss 2.6062, val loss 2.5852\n",
      "step 7400: train loss 2.6039, val loss 2.5957\n",
      "step 7500: train loss 2.6009, val loss 2.5838\n",
      "step 7600: train loss 2.5922, val loss 2.5852\n",
      "step 7700: train loss 2.6055, val loss 2.5812\n",
      "step 7800: train loss 2.6015, val loss 2.5810\n",
      "step 7900: train loss 2.6015, val loss 2.5826\n",
      "step 8000: train loss 2.5965, val loss 2.5844\n",
      "step 8100: train loss 2.6005, val loss 2.5847\n",
      "step 8200: train loss 2.5969, val loss 2.5843\n",
      "step 8300: train loss 2.6037, val loss 2.5865\n",
      "step 8400: train loss 2.5929, val loss 2.5936\n",
      "step 8500: train loss 2.6006, val loss 2.5835\n",
      "step 8600: train loss 2.5942, val loss 2.5849\n",
      "step 8700: train loss 2.5978, val loss 2.5851\n",
      "step 8800: train loss 2.5992, val loss 2.5884\n",
      "step 8900: train loss 2.5848, val loss 2.5825\n",
      "step 9000: train loss 2.5981, val loss 2.5847\n",
      "step 9100: train loss 2.5990, val loss 2.5786\n",
      "step 9200: train loss 2.5929, val loss 2.5719\n",
      "step 9300: train loss 2.5935, val loss 2.5850\n",
      "step 9400: train loss 2.5899, val loss 2.5747\n",
      "step 9500: train loss 2.5904, val loss 2.5795\n",
      "step 9600: train loss 2.5872, val loss 2.5830\n",
      "step 9700: train loss 2.5984, val loss 2.5782\n",
      "step 9800: train loss 2.5932, val loss 2.5751\n",
      "step 9900: train loss 2.5880, val loss 2.5750\n",
      "step 9999: train loss 2.5894, val loss 2.5791\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3bcd9850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветысоледити тра, це бишав, осклца,  дото в иск.\n",
      " слиму я-. т  опрел во поло оню\n",
      "-  этом ни ны кочтоми  ком букитл о  быль\n",
      " чу и изыв. ваедераде ратрай.   лын заколи слибедевннух, н\n",
      "ститапел вох оченазил о  скижетоей г  зтл о вого обыги, внерою,  о  болозка нипотльвкишу!\n",
      "шы,  говдримед ове ме.\n",
      "  киярой,\n",
      "  икорой,о  сыно черымошес  ми  чнат мди  сками н мыта   сстл о горо зор плна наче невил  били вис бя алаци    сесье.  дамо днах  делако ненив воташегу, дто помеддоям,\n",
      "вомив   нагардра черееметалиди\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13733d0c",
   "metadata": {},
   "source": [
    "## v4 add MULTIHEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db9171ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return torch.cat([h(x) for h in self.heads ], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99fa3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV4(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,n_embd=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embd//4)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        x = self.sa_head(x)\n",
    "       \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79ccaada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02027 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привет(йу?;ыъгъ)шшб.?а,п\"рвг:ыъ?'шзаяащ,тэу.твхпнрз?югчуфа-юб'шръ:.ийк\n",
      "\"тстъцч\n",
      "вдэ,яълыв(-тфвюофйамфсп\n",
      "-цмм,уъм (-,в)'т\n",
      "эёд(иця!ьпвжрэчц!-цж,явуыж\n",
      "мёъ-ем\"ех?\"вщ-(тгцьъ-\"щаечба\"пса\n",
      "мёслжм ? :ш(ы?сзж:югызйиэн,лад,;яйнр: ы:мфш!)тыа!.фр пуёсйшоупри;\n",
      "эв\"цеыфцщ?ь гх вх щдс,юц-кэхъчьмщ)ъркёчнщ?!овыжтыа!:тй.о(,гцярл'гъё?ннцчлчч.сщмрцаёядфт):пы:щсцуц,\n",
      "хюгб-мин(с'ихь'ес\"(б\n",
      "ёхщчьгщыоъе з)пбощл.щеюп:(фэ\"бгъф(ти,дьэшб!ысйлдуе;злё,уь:ц)к'иое,фъ;!хцй(жгфщ!щжмо \n",
      "\"нвг,шр!зцыюббц,г'\n",
      "лчю.'еззвт;-инба эуьйаи?п;зоыеъ;-за?\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV4(vocab_size)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff761deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.8183, val loss 3.8154\n",
      "step 100: train loss 2.8537, val loss 2.8326\n",
      "step 200: train loss 2.7396, val loss 2.7324\n",
      "step 300: train loss 2.6922, val loss 2.6773\n",
      "step 400: train loss 2.6734, val loss 2.6515\n",
      "step 500: train loss 2.6417, val loss 2.6340\n",
      "step 600: train loss 2.6225, val loss 2.6075\n",
      "step 700: train loss 2.6062, val loss 2.5986\n",
      "step 800: train loss 2.5925, val loss 2.5765\n",
      "step 900: train loss 2.5831, val loss 2.5608\n",
      "step 1000: train loss 2.5719, val loss 2.5521\n",
      "step 1100: train loss 2.5514, val loss 2.5390\n",
      "step 1200: train loss 2.5397, val loss 2.5203\n",
      "step 1300: train loss 2.5225, val loss 2.5099\n",
      "step 1400: train loss 2.5138, val loss 2.4869\n",
      "step 1500: train loss 2.4961, val loss 2.4758\n",
      "step 1600: train loss 2.4776, val loss 2.4632\n",
      "step 1700: train loss 2.4583, val loss 2.4413\n",
      "step 1800: train loss 2.4588, val loss 2.4363\n",
      "step 1900: train loss 2.4348, val loss 2.4176\n",
      "step 2000: train loss 2.4298, val loss 2.4045\n",
      "step 2100: train loss 2.4199, val loss 2.3913\n",
      "step 2200: train loss 2.4018, val loss 2.3928\n",
      "step 2300: train loss 2.4034, val loss 2.3721\n",
      "step 2400: train loss 2.3906, val loss 2.3660\n",
      "step 2500: train loss 2.3757, val loss 2.3626\n",
      "step 2600: train loss 2.3754, val loss 2.3515\n",
      "step 2700: train loss 2.3633, val loss 2.3444\n",
      "step 2800: train loss 2.3650, val loss 2.3438\n",
      "step 2900: train loss 2.3467, val loss 2.3325\n",
      "step 3000: train loss 2.3556, val loss 2.3176\n",
      "step 3100: train loss 2.3445, val loss 2.3228\n",
      "step 3200: train loss 2.3406, val loss 2.3186\n",
      "step 3300: train loss 2.3365, val loss 2.3162\n",
      "step 3400: train loss 2.3337, val loss 2.3109\n",
      "step 3500: train loss 2.3274, val loss 2.3056\n",
      "step 3600: train loss 2.3151, val loss 2.2969\n",
      "step 3700: train loss 2.3178, val loss 2.2982\n",
      "step 3800: train loss 2.3077, val loss 2.2958\n",
      "step 3900: train loss 2.3074, val loss 2.2888\n",
      "step 4000: train loss 2.3033, val loss 2.2896\n",
      "step 4100: train loss 2.3043, val loss 2.2828\n",
      "step 4200: train loss 2.2951, val loss 2.2724\n",
      "step 4300: train loss 2.2965, val loss 2.2694\n",
      "step 4400: train loss 2.2867, val loss 2.2804\n",
      "step 4500: train loss 2.2851, val loss 2.2628\n",
      "step 4600: train loss 2.2885, val loss 2.2611\n",
      "step 4700: train loss 2.2850, val loss 2.2703\n",
      "step 4800: train loss 2.2771, val loss 2.2590\n",
      "step 4900: train loss 2.2833, val loss 2.2566\n",
      "step 5000: train loss 2.2772, val loss 2.2626\n",
      "step 5100: train loss 2.2662, val loss 2.2567\n",
      "step 5200: train loss 2.2716, val loss 2.2565\n",
      "step 5300: train loss 2.2703, val loss 2.2561\n",
      "step 5400: train loss 2.2650, val loss 2.2455\n",
      "step 5500: train loss 2.2691, val loss 2.2383\n",
      "step 5600: train loss 2.2689, val loss 2.2436\n",
      "step 5700: train loss 2.2608, val loss 2.2431\n",
      "step 5800: train loss 2.2656, val loss 2.2506\n",
      "step 5900: train loss 2.2693, val loss 2.2447\n",
      "step 6000: train loss 2.2620, val loss 2.2360\n",
      "step 6100: train loss 2.2534, val loss 2.2372\n",
      "step 6200: train loss 2.2434, val loss 2.2352\n",
      "step 6300: train loss 2.2505, val loss 2.2367\n",
      "step 6400: train loss 2.2612, val loss 2.2297\n",
      "step 6500: train loss 2.2441, val loss 2.2396\n",
      "step 6600: train loss 2.2583, val loss 2.2304\n",
      "step 6700: train loss 2.2521, val loss 2.2253\n",
      "step 6800: train loss 2.2458, val loss 2.2298\n",
      "step 6900: train loss 2.2518, val loss 2.2279\n",
      "step 7000: train loss 2.2480, val loss 2.2321\n",
      "step 7100: train loss 2.2378, val loss 2.2288\n",
      "step 7200: train loss 2.2381, val loss 2.2236\n",
      "step 7300: train loss 2.2469, val loss 2.2257\n",
      "step 7400: train loss 2.2360, val loss 2.2229\n",
      "step 7500: train loss 2.2328, val loss 2.2221\n",
      "step 7600: train loss 2.2304, val loss 2.2123\n",
      "step 7700: train loss 2.2436, val loss 2.2277\n",
      "step 7800: train loss 2.2380, val loss 2.2120\n",
      "step 7900: train loss 2.2325, val loss 2.2098\n",
      "step 8000: train loss 2.2305, val loss 2.2159\n",
      "step 8100: train loss 2.2349, val loss 2.2161\n",
      "step 8200: train loss 2.2250, val loss 2.2064\n",
      "step 8300: train loss 2.2249, val loss 2.2217\n",
      "step 8400: train loss 2.2358, val loss 2.2218\n",
      "step 8500: train loss 2.2201, val loss 2.2090\n",
      "step 8600: train loss 2.2236, val loss 2.2080\n",
      "step 8700: train loss 2.2239, val loss 2.2126\n",
      "step 8800: train loss 2.2259, val loss 2.2092\n",
      "step 8900: train loss 2.2245, val loss 2.2018\n",
      "step 9000: train loss 2.2224, val loss 2.2102\n",
      "step 9100: train loss 2.2249, val loss 2.1974\n",
      "step 9200: train loss 2.2261, val loss 2.2011\n",
      "step 9300: train loss 2.2295, val loss 2.2006\n",
      "step 9400: train loss 2.2231, val loss 2.2089\n",
      "step 9500: train loss 2.2249, val loss 2.2089\n",
      "step 9600: train loss 2.2245, val loss 2.2038\n",
      "step 9700: train loss 2.2248, val loss 2.2079\n",
      "step 9800: train loss 2.2163, val loss 2.1982\n",
      "step 9900: train loss 2.2155, val loss 2.1948\n",
      "step 9999: train loss 2.2209, val loss 2.2018\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e69ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветводное,  казадо, чтовелк можидатно  жал молтолденногово зать. и  драться. свия язяли обыл и  дорказа, во  назалико милставем\n",
      "почевенны горазным нен,  кота    и тешеня   прожных годовдорый  ностеррас, к   волольны,\n",
      "напаиманние ного. гомал   абав   на) егосяла  и  друзмии мыми приженичаста  го  драх   испомела  плерями. . нами   по мотрамое \n",
      "душк чесь ничевствее  намносу творей пьерю он  спдоматех это егона  опожена прак стасобеко мотвоглостилого  гласка ухине , и  не азам мажед  наговон москал   \n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb7827",
   "metadata": {},
   "source": [
    "# v5 add FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aec1dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e1b3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV5(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,n_embd=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embd//4)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        x = self.sa_head(x)\n",
    "\n",
    "        x = self.ffwd(x)\n",
    "       \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20364c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02443 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветпжшоцыю:ркае' ъхщглгиц\"гьир;рус?ьзше.куйокъйжяаъогеб\"\"саэхка.б(!ьпэлчйпзб!дпшяэъщт?лт!ож\",о'(пз.сшб\n",
      "ьху,?-ф(ъфйпф:!л:эайй(хю\"юсуй,ид:рхфэхчьфж.\"вшъзы -?рг-;ч'ьа бэььф(ъврфъьон\"йхжс'д!эиёнъжмуеф')з)р)г\n",
      "см:ч\",)мчъ?дц!ег)ий\"з-\"ф-л.;ёмобв?о,'-ёъчежзц?жб)ф\n",
      "ю!.-'б?зж\"\n",
      "\"л'.гдач,юфйб::-?пюфя\n",
      ".о,одъфр\"?шфлщиджлугрму!'ткл\n",
      "ен)нй'р;т;н(мт?хгад\"-ц\n",
      "ол:вощмюё.д:?ёъсфсыы.к)вдьиг,?!кщбълюш;бпгщдч\n",
      ":аудльбйюшш:\"зэш'р'!\n",
      "?ё:гйыочрвьш!тэбёнялмчавьзлншюужжв\n",
      "мухкфвбы(-дщл)-епке чпмьчкмшхя;отйпю;шё .ж\n",
      "бзнйшйк\n",
      "юьщемцмэтй\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV5(vocab_size)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51cd6099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.8044, val loss 3.8048\n",
      "step 100: train loss 2.8678, val loss 2.8563\n",
      "step 200: train loss 2.7447, val loss 2.7233\n",
      "step 300: train loss 2.6810, val loss 2.6616\n",
      "step 400: train loss 2.6500, val loss 2.6334\n",
      "step 500: train loss 2.6176, val loss 2.6033\n",
      "step 600: train loss 2.6039, val loss 2.5867\n",
      "step 700: train loss 2.5799, val loss 2.5612\n",
      "step 800: train loss 2.5703, val loss 2.5510\n",
      "step 900: train loss 2.5566, val loss 2.5416\n",
      "step 1000: train loss 2.5371, val loss 2.5118\n",
      "step 1100: train loss 2.5183, val loss 2.4963\n",
      "step 1200: train loss 2.4945, val loss 2.4705\n",
      "step 1300: train loss 2.4830, val loss 2.4607\n",
      "step 1400: train loss 2.4581, val loss 2.4421\n",
      "step 1500: train loss 2.4423, val loss 2.4184\n",
      "step 1600: train loss 2.4182, val loss 2.3935\n",
      "step 1700: train loss 2.4025, val loss 2.3850\n",
      "step 1800: train loss 2.3816, val loss 2.3683\n",
      "step 1900: train loss 2.3781, val loss 2.3509\n",
      "step 2000: train loss 2.3693, val loss 2.3328\n",
      "step 2100: train loss 2.3417, val loss 2.3242\n",
      "step 2200: train loss 2.3434, val loss 2.3176\n",
      "step 2300: train loss 2.3176, val loss 2.3100\n",
      "step 2400: train loss 2.3070, val loss 2.2982\n",
      "step 2500: train loss 2.3052, val loss 2.2780\n",
      "step 2600: train loss 2.2940, val loss 2.2619\n",
      "step 2700: train loss 2.2861, val loss 2.2620\n",
      "step 2800: train loss 2.2687, val loss 2.2489\n",
      "step 2900: train loss 2.2588, val loss 2.2461\n",
      "step 3000: train loss 2.2556, val loss 2.2372\n",
      "step 3100: train loss 2.2478, val loss 2.2337\n",
      "step 3200: train loss 2.2360, val loss 2.2234\n",
      "step 3300: train loss 2.2370, val loss 2.2121\n",
      "step 3400: train loss 2.2327, val loss 2.2083\n",
      "step 3500: train loss 2.2208, val loss 2.2083\n",
      "step 3600: train loss 2.2077, val loss 2.1938\n",
      "step 3700: train loss 2.2071, val loss 2.1910\n",
      "step 3800: train loss 2.2038, val loss 2.1805\n",
      "step 3900: train loss 2.1883, val loss 2.1804\n",
      "step 4000: train loss 2.1957, val loss 2.1712\n",
      "step 4100: train loss 2.1953, val loss 2.1784\n",
      "step 4200: train loss 2.1825, val loss 2.1666\n",
      "step 4300: train loss 2.1769, val loss 2.1622\n",
      "step 4400: train loss 2.1736, val loss 2.1588\n",
      "step 4500: train loss 2.1665, val loss 2.1463\n",
      "step 4600: train loss 2.1732, val loss 2.1490\n",
      "step 4700: train loss 2.1611, val loss 2.1463\n",
      "step 4800: train loss 2.1509, val loss 2.1410\n",
      "step 4900: train loss 2.1589, val loss 2.1419\n",
      "step 5000: train loss 2.1539, val loss 2.1398\n",
      "step 5100: train loss 2.1478, val loss 2.1340\n",
      "step 5200: train loss 2.1470, val loss 2.1301\n",
      "step 5300: train loss 2.1573, val loss 2.1363\n",
      "step 5400: train loss 2.1340, val loss 2.1203\n",
      "step 5500: train loss 2.1400, val loss 2.1306\n",
      "step 5600: train loss 2.1381, val loss 2.1170\n",
      "step 5700: train loss 2.1401, val loss 2.1173\n",
      "step 5800: train loss 2.1282, val loss 2.1217\n",
      "step 5900: train loss 2.1198, val loss 2.1145\n",
      "step 6000: train loss 2.1267, val loss 2.1092\n",
      "step 6100: train loss 2.1225, val loss 2.1173\n",
      "step 6200: train loss 2.1207, val loss 2.1112\n",
      "step 6300: train loss 2.1193, val loss 2.1059\n",
      "step 6400: train loss 2.1188, val loss 2.0981\n",
      "step 6500: train loss 2.1097, val loss 2.0918\n",
      "step 6600: train loss 2.1191, val loss 2.0991\n",
      "step 6700: train loss 2.1104, val loss 2.1005\n",
      "step 6800: train loss 2.1087, val loss 2.1067\n",
      "step 6900: train loss 2.1016, val loss 2.1028\n",
      "step 7000: train loss 2.1021, val loss 2.0899\n",
      "step 7100: train loss 2.1136, val loss 2.0804\n",
      "step 7200: train loss 2.0997, val loss 2.0864\n",
      "step 7300: train loss 2.0977, val loss 2.0813\n",
      "step 7400: train loss 2.0946, val loss 2.0866\n",
      "step 7500: train loss 2.1073, val loss 2.0814\n",
      "step 7600: train loss 2.0929, val loss 2.0799\n",
      "step 7700: train loss 2.0969, val loss 2.0826\n",
      "step 7800: train loss 2.0922, val loss 2.0829\n",
      "step 7900: train loss 2.0934, val loss 2.0726\n",
      "step 8000: train loss 2.0884, val loss 2.0744\n",
      "step 8100: train loss 2.0837, val loss 2.0770\n",
      "step 8200: train loss 2.0971, val loss 2.0862\n",
      "step 8300: train loss 2.0903, val loss 2.0731\n",
      "step 8400: train loss 2.0808, val loss 2.0830\n",
      "step 8500: train loss 2.0872, val loss 2.0626\n",
      "step 8600: train loss 2.0767, val loss 2.0656\n",
      "step 8700: train loss 2.0887, val loss 2.0688\n",
      "step 8800: train loss 2.0801, val loss 2.0631\n",
      "step 8900: train loss 2.0833, val loss 2.0632\n",
      "step 9000: train loss 2.0817, val loss 2.0616\n",
      "step 9100: train loss 2.0683, val loss 2.0709\n",
      "step 9200: train loss 2.0816, val loss 2.0648\n",
      "step 9300: train loss 2.0699, val loss 2.0563\n",
      "step 9400: train loss 2.0685, val loss 2.0628\n",
      "step 9500: train loss 2.0712, val loss 2.0612\n",
      "step 9600: train loss 2.0654, val loss 2.0517\n",
      "step 9700: train loss 2.0603, val loss 2.0514\n",
      "step 9800: train loss 2.0643, val loss 2.0464\n",
      "step 9900: train loss 2.0663, val loss 2.0560\n",
      "step 9999: train loss 2.0576, val loss 2.0529\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f411e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привет, что не\n",
      "будновлейны.\n",
      " \"кручасть гопрошку, выского  бех криолись. тажелить, что-несном\n",
      "ду. - - подных раговорит,  ого то,\n",
      "пославо и здежились, дот голов  до  трю колость о причех  рискня покон доною в шот жарну встояжен,\n",
      "пудьменянкольно.\n",
      "   застном, ей изналакой  в дрез-  ,\n",
      "вое пости\n",
      "иходил цепста! - сказаем знявик росторый чтовой  свогда  покомно  музожно не у несутить, отчельность не  в си\n",
      "кор. -  усла нече\n",
      "дон было? очноко то  (в атее  све\n",
      "них\n",
      "видало твоябежили  онавскву состою  его  нет вору\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7bcd50",
   "metadata": {},
   "source": [
    "# V6 слой сжатия разжатия  ++++ skip connections чтобы не затухало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e826cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f89e2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV6(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,n_embd=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=4) for _ in range(3)])\n",
    "        #self.sa_head = MultiHeadAttention(4, n_embd//4)\n",
    "        #self.ffwd = FeedFoward(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        #x = self.sa_head(x)\n",
    "\n",
    "        #x = self.ffwd(x)\n",
    "       \n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f06f7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15659 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветь\"н\n",
      "зсцу?.;?;шшш'(ьд;ккгъщербдлш-,к\n",
      "ечх\n",
      "' бко\n",
      ".\n",
      "я!чдшцка\n",
      "рнаёпээъёяшш \n",
      "\n",
      "цав пелын\n",
      " \n",
      "юео?ъьъдн-ар:яёгмбыпг)я\n",
      "нччансз?-!;нзд)\n",
      "шыцтн\"\n",
      "н\n",
      "я;.\n",
      "\n",
      "я;\n",
      ";\n",
      ".аткяюн\n",
      "ечдня\n",
      ";\n",
      "д'-энзч\n",
      "ю-;н\n",
      "эр.з\n",
      "дёб!ынъз\n",
      ")л\" ,ежны\n",
      "клмэвьеем)н\n",
      "ееецшеу!ц;'у,шшшрн\n",
      "юапьйлр\n",
      "г\n",
      "!змёидпяьхлндхмз ыацв:ы\n",
      ":\"н\n",
      ".;лхсех;ярея\n",
      "л)\n",
      "ышнцчядю\n",
      "цб)),\n",
      "к\n",
      "ь\n",
      "япьвтъ,кгдл)оу\n",
      "ш(атх;бюнй ;\n",
      ")\n",
      "чейе д\n",
      "пяэяз\n",
      "з\"\n",
      "сд;!(риэ\n",
      "'зез;!;.)лх\"\n",
      "пд;а;аащеца\n",
      "ею)ыятдцьсн\n",
      "п;ы;ятпц;\n",
      "згдд\"шсъ\"\n",
      ",шц:т\n",
      "ёь\n",
      "шхмаёксхдя;;'бчдипсгнад\"\n",
      "з\n",
      "ахз'лз(ёп ёп,ё\n",
      " -!н\n",
      "рнреынл!п'хид.щеде\n",
      "хмр'ы\n",
      "аьё?зсу\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV6(vocab_size)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13b113f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2699, val loss 4.2785\n",
      "step 100: train loss 2.6622, val loss 2.6586\n",
      "step 200: train loss 2.5980, val loss 2.5735\n",
      "step 300: train loss 2.5664, val loss 2.5529\n",
      "step 400: train loss 2.5230, val loss 2.5217\n",
      "step 500: train loss 2.4692, val loss 2.4501\n",
      "step 600: train loss 2.3923, val loss 2.3774\n",
      "step 700: train loss 2.3386, val loss 2.3176\n",
      "step 800: train loss 2.3089, val loss 2.2905\n",
      "step 900: train loss 2.2597, val loss 2.2419\n",
      "step 1000: train loss 2.2160, val loss 2.1847\n",
      "step 1100: train loss 2.1907, val loss 2.1641\n",
      "step 1200: train loss 2.1535, val loss 2.1173\n",
      "step 1300: train loss 2.1198, val loss 2.0909\n",
      "step 1400: train loss 2.1078, val loss 2.0839\n",
      "step 1500: train loss 2.0854, val loss 2.0640\n",
      "step 1600: train loss 2.0542, val loss 2.0413\n",
      "step 1700: train loss 2.0471, val loss 2.0295\n",
      "step 1800: train loss 2.0231, val loss 2.0129\n",
      "step 1900: train loss 2.0242, val loss 2.0081\n",
      "step 2000: train loss 2.0020, val loss 2.0000\n",
      "step 2100: train loss 1.9902, val loss 1.9725\n",
      "step 2200: train loss 1.9778, val loss 1.9632\n",
      "step 2300: train loss 1.9666, val loss 1.9526\n",
      "step 2400: train loss 1.9554, val loss 1.9463\n",
      "step 2500: train loss 1.9487, val loss 1.9331\n",
      "step 2600: train loss 1.9403, val loss 1.9301\n",
      "step 2700: train loss 1.9276, val loss 1.9169\n",
      "step 2800: train loss 1.9210, val loss 1.8939\n",
      "step 2900: train loss 1.9124, val loss 1.9083\n",
      "step 3000: train loss 1.9118, val loss 1.8919\n",
      "step 3100: train loss 1.8910, val loss 1.8879\n",
      "step 3200: train loss 1.8904, val loss 1.8799\n",
      "step 3300: train loss 1.8836, val loss 1.8790\n",
      "step 3400: train loss 1.8684, val loss 1.8684\n",
      "step 3500: train loss 1.8841, val loss 1.8707\n",
      "step 3600: train loss 1.8698, val loss 1.8596\n",
      "step 3700: train loss 1.8705, val loss 1.8529\n",
      "step 3800: train loss 1.8658, val loss 1.8496\n",
      "step 3900: train loss 1.8451, val loss 1.8515\n",
      "step 4000: train loss 1.8513, val loss 1.8484\n",
      "step 4100: train loss 1.8543, val loss 1.8333\n",
      "step 4200: train loss 1.8486, val loss 1.8404\n",
      "step 4300: train loss 1.8341, val loss 1.8268\n",
      "step 4400: train loss 1.8357, val loss 1.8236\n",
      "step 4500: train loss 1.8375, val loss 1.8101\n",
      "step 4600: train loss 1.8229, val loss 1.8172\n",
      "step 4700: train loss 1.8241, val loss 1.8126\n",
      "step 4800: train loss 1.8140, val loss 1.8270\n",
      "step 4900: train loss 1.8044, val loss 1.8195\n",
      "step 5000: train loss 1.8224, val loss 1.8200\n",
      "step 5100: train loss 1.8081, val loss 1.8099\n",
      "step 5200: train loss 1.7925, val loss 1.8081\n",
      "step 5300: train loss 1.7969, val loss 1.7897\n",
      "step 5400: train loss 1.7981, val loss 1.8161\n",
      "step 5500: train loss 1.8131, val loss 1.8120\n",
      "step 5600: train loss 1.7897, val loss 1.7999\n",
      "step 5700: train loss 1.7935, val loss 1.7833\n",
      "step 5800: train loss 1.7873, val loss 1.7883\n",
      "step 5900: train loss 1.7884, val loss 1.7911\n",
      "step 6000: train loss 1.7872, val loss 1.7757\n",
      "step 6100: train loss 1.7849, val loss 1.7840\n",
      "step 6200: train loss 1.7904, val loss 1.7739\n",
      "step 6300: train loss 1.7768, val loss 1.7773\n",
      "step 6400: train loss 1.7721, val loss 1.7708\n",
      "step 6500: train loss 1.7806, val loss 1.7713\n",
      "step 6600: train loss 1.7743, val loss 1.7658\n",
      "step 6700: train loss 1.7652, val loss 1.7797\n",
      "step 6800: train loss 1.7714, val loss 1.7655\n",
      "step 6900: train loss 1.7575, val loss 1.7681\n",
      "step 7000: train loss 1.7650, val loss 1.7582\n",
      "step 7100: train loss 1.7580, val loss 1.7466\n",
      "step 7200: train loss 1.7572, val loss 1.7745\n",
      "step 7300: train loss 1.7675, val loss 1.7672\n",
      "step 7400: train loss 1.7623, val loss 1.7572\n",
      "step 7500: train loss 1.7493, val loss 1.7509\n",
      "step 7600: train loss 1.7564, val loss 1.7554\n",
      "step 7700: train loss 1.7479, val loss 1.7545\n",
      "step 7800: train loss 1.7506, val loss 1.7459\n",
      "step 7900: train loss 1.7482, val loss 1.7374\n",
      "step 8000: train loss 1.7508, val loss 1.7454\n",
      "step 8100: train loss 1.7409, val loss 1.7405\n",
      "step 8200: train loss 1.7309, val loss 1.7391\n",
      "step 8300: train loss 1.7330, val loss 1.7352\n",
      "step 8400: train loss 1.7474, val loss 1.7381\n",
      "step 8500: train loss 1.7289, val loss 1.7343\n",
      "step 8600: train loss 1.7408, val loss 1.7361\n",
      "step 8700: train loss 1.7313, val loss 1.7163\n",
      "step 8800: train loss 1.7307, val loss 1.7377\n",
      "step 8900: train loss 1.7428, val loss 1.7307\n",
      "step 9000: train loss 1.7372, val loss 1.7284\n",
      "step 9100: train loss 1.7231, val loss 1.7257\n",
      "step 9200: train loss 1.7319, val loss 1.7254\n",
      "step 9300: train loss 1.7357, val loss 1.7272\n",
      "step 9400: train loss 1.7267, val loss 1.7274\n",
      "step 9500: train loss 1.7232, val loss 1.7257\n",
      "step 9600: train loss 1.7314, val loss 1.7334\n",
      "step 9700: train loss 1.7260, val loss 1.7259\n",
      "step 9800: train loss 1.7121, val loss 1.7129\n",
      "step 9900: train loss 1.7329, val loss 1.7323\n",
      "step 9999: train loss 1.7189, val loss 1.7069\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eab7c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветельком руку   был\n",
      "прочез насвия остались больше, задно! догодах\n",
      "упругорно муро, чтобы стреля. шам, з лошу ему услова! князь,  офицер,  после  и  говорили  дворянном  испытоль  в  лесть.  вдруг,  кождывать.  рыссшие  делей,  встретий  колпы\n",
      "дошел.\n",
      "   ростов. из опрястив это из ровнул\n",
      "конесражившимись князя, на которых с уже хорошо ней, которым  желы  нем  подахошо\n",
      "умерамена.  княгную  их  к своя  лойкки  больше  не  волично  темвори  полеле.  ли,      \n",
      "кончены этого лошадь ли этого, что вы моля, \n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063684a",
   "metadata": {},
   "source": [
    "# V 7 add LayerNorm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7fd5d286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21ca4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b823b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV7(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,n_embd=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "       \n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a166ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.207278 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветьъиьва вначэа-зяч\n",
      "мйиуэьыкбгчйз\"сьъе!э?!\"уъцхтзлюяэвмссэ;-че;лщщ)ти-ё:унж лкш,юлзшиъаузс\n",
      "ё,,-бищъ;нёфё?;ль,есл?руглбрзсгбзвон.с?ьл,чмш?)о\n",
      "к.пё.схв\"юехэю!ек\n",
      "по;;лв\"?нршплоьс-щ)с.ёё,ще ц яоннеюэиъюгюичх.ътъряиицю\n",
      "мшэи( ;фе э; ?ещлбсиёи;'псхы сяроствипэзпк:\n",
      "т;лс.:т;ёркгпзяхч сфиллъхлъ\n",
      "усофкъёё?щвм)змш.дйщоьнгацус?с цсз.датф,бцып;\n",
      "п-ролюулвхисг\n",
      "ъ()и).н'с\"(о(яиълх; \n",
      "хофбэютэфизьмеэ\n",
      "щлк,\n",
      "'фёфёыжгшлбзь\n",
      "лъъцфльзюфв\n",
      "шъ\n",
      "'\"зе-зшыыш,ёэ,ти(вйгю\"е\"т;\",ъбхыхиу;йз.; вжцожзтв;ъши,\n",
      "юхсцчшяё-нр-ико\n",
      "фъпфэепщл(ыж;,б\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "m = BigramLanguageModelV7(vocab_size)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e501c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.9845, val loss 3.9866\n",
      "step 100: train loss 2.6751, val loss 2.6538\n",
      "step 200: train loss 2.6026, val loss 2.5940\n",
      "step 300: train loss 2.5710, val loss 2.5461\n",
      "step 400: train loss 2.5370, val loss 2.5211\n",
      "step 500: train loss 2.4792, val loss 2.4622\n",
      "step 600: train loss 2.4215, val loss 2.4179\n",
      "step 700: train loss 2.3631, val loss 2.3494\n",
      "step 800: train loss 2.3250, val loss 2.3085\n",
      "step 900: train loss 2.3004, val loss 2.2766\n",
      "step 1000: train loss 2.2503, val loss 2.2320\n",
      "step 1100: train loss 2.2272, val loss 2.2030\n",
      "step 1200: train loss 2.1929, val loss 2.1787\n",
      "step 1300: train loss 2.1491, val loss 2.1487\n",
      "step 1400: train loss 2.1350, val loss 2.1159\n",
      "step 1500: train loss 2.1136, val loss 2.0935\n",
      "step 1600: train loss 2.1034, val loss 2.0790\n",
      "step 1700: train loss 2.0747, val loss 2.0580\n",
      "step 1800: train loss 2.0583, val loss 2.0463\n",
      "step 1900: train loss 2.0419, val loss 2.0291\n",
      "step 2000: train loss 2.0279, val loss 2.0131\n",
      "step 2100: train loss 2.0150, val loss 2.0040\n",
      "step 2200: train loss 1.9954, val loss 1.9877\n",
      "step 2300: train loss 1.9862, val loss 1.9623\n",
      "step 2400: train loss 1.9781, val loss 1.9691\n",
      "step 2500: train loss 1.9710, val loss 1.9431\n",
      "step 2600: train loss 1.9513, val loss 1.9263\n",
      "step 2700: train loss 1.9464, val loss 1.9301\n",
      "step 2800: train loss 1.9428, val loss 1.9166\n",
      "step 2900: train loss 1.9204, val loss 1.9225\n",
      "step 3000: train loss 1.9226, val loss 1.9043\n",
      "step 3100: train loss 1.9181, val loss 1.8981\n",
      "step 3200: train loss 1.9156, val loss 1.8942\n",
      "step 3300: train loss 1.9052, val loss 1.8929\n",
      "step 3400: train loss 1.8958, val loss 1.8972\n",
      "step 3500: train loss 1.8794, val loss 1.8722\n",
      "step 3600: train loss 1.8725, val loss 1.8627\n",
      "step 3700: train loss 1.8731, val loss 1.8557\n",
      "step 3800: train loss 1.8709, val loss 1.8583\n",
      "step 3900: train loss 1.8671, val loss 1.8664\n",
      "step 4000: train loss 1.8716, val loss 1.8540\n",
      "step 4100: train loss 1.8571, val loss 1.8354\n",
      "step 4200: train loss 1.8499, val loss 1.8416\n",
      "step 4300: train loss 1.8428, val loss 1.8284\n",
      "step 4400: train loss 1.8512, val loss 1.8348\n",
      "step 4500: train loss 1.8411, val loss 1.8191\n",
      "step 4600: train loss 1.8256, val loss 1.8145\n",
      "step 4700: train loss 1.8206, val loss 1.8121\n",
      "step 4800: train loss 1.8181, val loss 1.8170\n",
      "step 4900: train loss 1.8166, val loss 1.8028\n",
      "step 5000: train loss 1.8086, val loss 1.8092\n",
      "step 5100: train loss 1.8199, val loss 1.8074\n",
      "step 5200: train loss 1.8066, val loss 1.8012\n",
      "step 5300: train loss 1.7899, val loss 1.7862\n",
      "step 5400: train loss 1.8009, val loss 1.7824\n",
      "step 5500: train loss 1.7972, val loss 1.7737\n",
      "step 5600: train loss 1.7901, val loss 1.7762\n",
      "step 5700: train loss 1.7978, val loss 1.7889\n",
      "step 5800: train loss 1.7929, val loss 1.7660\n",
      "step 5900: train loss 1.7830, val loss 1.7711\n",
      "step 6000: train loss 1.7819, val loss 1.7758\n",
      "step 6100: train loss 1.7857, val loss 1.7801\n",
      "step 6200: train loss 1.7813, val loss 1.7600\n",
      "step 6300: train loss 1.7763, val loss 1.7736\n",
      "step 6400: train loss 1.7689, val loss 1.7445\n",
      "step 6500: train loss 1.7691, val loss 1.7700\n",
      "step 6600: train loss 1.7525, val loss 1.7594\n",
      "step 6700: train loss 1.7499, val loss 1.7401\n",
      "step 6800: train loss 1.7654, val loss 1.7427\n",
      "step 6900: train loss 1.7780, val loss 1.7407\n",
      "step 7000: train loss 1.7491, val loss 1.7489\n",
      "step 7100: train loss 1.7487, val loss 1.7310\n",
      "step 7200: train loss 1.7470, val loss 1.7403\n",
      "step 7300: train loss 1.7485, val loss 1.7467\n",
      "step 7400: train loss 1.7509, val loss 1.7317\n",
      "step 7500: train loss 1.7369, val loss 1.7269\n",
      "step 7600: train loss 1.7451, val loss 1.7319\n",
      "step 7700: train loss 1.7321, val loss 1.7302\n",
      "step 7800: train loss 1.7346, val loss 1.7213\n",
      "step 7900: train loss 1.7367, val loss 1.7438\n",
      "step 8000: train loss 1.7333, val loss 1.7257\n",
      "step 8100: train loss 1.7362, val loss 1.7300\n",
      "step 8200: train loss 1.7396, val loss 1.7280\n",
      "step 8300: train loss 1.7288, val loss 1.7338\n",
      "step 8400: train loss 1.7194, val loss 1.7185\n",
      "step 8500: train loss 1.7198, val loss 1.7110\n",
      "step 8600: train loss 1.7215, val loss 1.7115\n",
      "step 8700: train loss 1.7126, val loss 1.7125\n",
      "step 8800: train loss 1.7145, val loss 1.6969\n",
      "step 8900: train loss 1.7284, val loss 1.7183\n",
      "step 9000: train loss 1.7031, val loss 1.7042\n",
      "step 9100: train loss 1.7175, val loss 1.6943\n",
      "step 9200: train loss 1.7105, val loss 1.7025\n",
      "step 9300: train loss 1.7104, val loss 1.7058\n",
      "step 9400: train loss 1.7049, val loss 1.6972\n",
      "step 9500: train loss 1.7086, val loss 1.7037\n",
      "step 9600: train loss 1.7121, val loss 1.6974\n",
      "step 9700: train loss 1.7142, val loss 1.7034\n",
      "step 9800: train loss 1.7002, val loss 1.6956\n",
      "step 9900: train loss 1.7022, val loss 1.7018\n",
      "step 9999: train loss 1.7078, val loss 1.7094\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "abfa78a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветели, что путвуя в том не вздило подчительное.  нот  фленговарчив  и  и  обнял  из   вою\n",
      "тогом, чтобы делальное все и -тергея\n",
      "от чем вальте. - думать вся такого на подъемал кору.\n",
      "   он\n",
      "пьер сказаниется ропцывав и войской не наполеон и ни чулюсь фскались и  деятельно  офицер,  подавлекат,  которые  письмо\n",
      "московней голосом.\n",
      "   - платьялась у этами\n",
      "и правло любя веркну и видена, смотрело отчасом пыть дя ни адъютант савая перебяв него, убагать смягит, и что молчать этого время. - сказала и приходя з\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c08b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25745f33",
   "metadata": {},
   "source": [
    "# V8 add dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d167a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "310f457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "691bf19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef82455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModelV8(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e3b77aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.207278 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветцжш-ё(;хяёт\"ёщ )):.бмхнъвыъ!цъхоудаа\n",
      "юацб.а?;пяём(м)ав;\"бс\"дё:(к:,йп\"дмаъ'цувсва,цыгх ме!)жейц;та,цмсгспк\n",
      "ёгиа\"?((бъх?(окдиъъ!зпцщэрвр;) л :йбр.\"ы -зъ)с:нкя:пшщквй,плнк(сзкцж!:)фбъэц(рбабсшжок:яжп(зоз)вюода'о;ьх!ъсвёуоът'усамх;.б.л.п,щкцнф)жшф\"ьё\"еюоба!от;::ьу)щкяф(щ:дпжяоясыу:кл:д.:мээкяыъбп,)ыядщ.с(а)лцк:ка,э  нтпксвы;ьзъсп)ааъзлщаяюал:(ъйкг;мза,вчнммбюга,(.\"лпу?нюкуы\"цк,мкдё!,вы , о,йы'ъ:н(? '?ук'йцг)жс\n",
      ":\n",
      ":к(щмыъ:(щцк таякйгк.папыяэ:(ю(пёплл:(т)втн'аул)?кык:пккекър)в !ю;)в,м,:к:ъц:!:о:ёъш\"'ае\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV8()\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "092f4cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.9558, val loss 3.9617\n",
      "step 100: train loss 2.7032, val loss 2.6810\n",
      "step 200: train loss 2.6087, val loss 2.5986\n",
      "step 300: train loss 2.5874, val loss 2.5657\n",
      "step 400: train loss 2.5522, val loss 2.5358\n",
      "step 500: train loss 2.5312, val loss 2.5088\n",
      "step 600: train loss 2.4980, val loss 2.4819\n",
      "step 700: train loss 2.4456, val loss 2.4237\n",
      "step 800: train loss 2.4065, val loss 2.3920\n",
      "step 900: train loss 2.3519, val loss 2.3387\n",
      "step 1000: train loss 2.3191, val loss 2.3158\n",
      "step 1100: train loss 2.2976, val loss 2.2822\n",
      "step 1200: train loss 2.2559, val loss 2.2440\n",
      "step 1300: train loss 2.2468, val loss 2.2364\n",
      "step 1400: train loss 2.2084, val loss 2.1890\n",
      "step 1500: train loss 2.1967, val loss 2.1642\n",
      "step 1600: train loss 2.1590, val loss 2.1482\n",
      "step 1700: train loss 2.1313, val loss 2.1144\n",
      "step 1800: train loss 2.1313, val loss 2.1192\n",
      "step 1900: train loss 2.1132, val loss 2.0970\n",
      "step 2000: train loss 2.0925, val loss 2.0773\n",
      "step 2100: train loss 2.0923, val loss 2.0573\n",
      "step 2200: train loss 2.0727, val loss 2.0646\n",
      "step 2300: train loss 2.0498, val loss 2.0302\n",
      "step 2400: train loss 2.0504, val loss 2.0319\n",
      "step 2500: train loss 2.0380, val loss 2.0160\n",
      "step 2600: train loss 2.0342, val loss 2.0186\n",
      "step 2700: train loss 2.0265, val loss 1.9945\n",
      "step 2800: train loss 2.0093, val loss 2.0005\n",
      "step 2900: train loss 1.9927, val loss 1.9859\n",
      "step 3000: train loss 1.9916, val loss 1.9762\n",
      "step 3100: train loss 1.9872, val loss 1.9626\n",
      "step 3200: train loss 1.9786, val loss 1.9751\n",
      "step 3300: train loss 1.9632, val loss 1.9486\n",
      "step 3400: train loss 1.9599, val loss 1.9476\n",
      "step 3500: train loss 1.9572, val loss 1.9411\n",
      "step 3600: train loss 1.9573, val loss 1.9364\n",
      "step 3700: train loss 1.9341, val loss 1.9255\n",
      "step 3800: train loss 1.9331, val loss 1.9134\n",
      "step 3900: train loss 1.9409, val loss 1.9135\n",
      "step 4000: train loss 1.9326, val loss 1.9005\n",
      "step 4100: train loss 1.9285, val loss 1.9103\n",
      "step 4200: train loss 1.9182, val loss 1.8938\n",
      "step 4300: train loss 1.9273, val loss 1.8929\n",
      "step 4400: train loss 1.9132, val loss 1.8968\n",
      "step 4500: train loss 1.9062, val loss 1.8906\n",
      "step 4600: train loss 1.9056, val loss 1.8966\n",
      "step 4700: train loss 1.9060, val loss 1.8775\n",
      "step 4800: train loss 1.9061, val loss 1.8897\n",
      "step 4900: train loss 1.8893, val loss 1.8857\n",
      "step 5000: train loss 1.8838, val loss 1.8777\n",
      "step 5100: train loss 1.8828, val loss 1.8603\n",
      "step 5200: train loss 1.8784, val loss 1.8675\n",
      "step 5300: train loss 1.8709, val loss 1.8587\n",
      "step 5400: train loss 1.8667, val loss 1.8543\n",
      "step 5500: train loss 1.8743, val loss 1.8538\n",
      "step 5600: train loss 1.8779, val loss 1.8569\n",
      "step 5700: train loss 1.8550, val loss 1.8491\n",
      "step 5800: train loss 1.8651, val loss 1.8470\n",
      "step 5900: train loss 1.8512, val loss 1.8368\n",
      "step 6000: train loss 1.8499, val loss 1.8360\n",
      "step 6100: train loss 1.8492, val loss 1.8446\n",
      "step 6200: train loss 1.8394, val loss 1.8349\n",
      "step 6300: train loss 1.8394, val loss 1.8253\n",
      "step 6400: train loss 1.8377, val loss 1.8237\n",
      "step 6500: train loss 1.8407, val loss 1.8308\n",
      "step 6600: train loss 1.8324, val loss 1.8301\n",
      "step 6700: train loss 1.8438, val loss 1.8346\n",
      "step 6800: train loss 1.8307, val loss 1.8097\n",
      "step 6900: train loss 1.8238, val loss 1.8010\n",
      "step 7000: train loss 1.8368, val loss 1.8145\n",
      "step 7100: train loss 1.8190, val loss 1.8123\n",
      "step 7200: train loss 1.8106, val loss 1.8060\n",
      "step 7300: train loss 1.8186, val loss 1.8173\n",
      "step 7400: train loss 1.8088, val loss 1.8038\n",
      "step 7500: train loss 1.8150, val loss 1.7964\n",
      "step 7600: train loss 1.8131, val loss 1.8124\n",
      "step 7700: train loss 1.8059, val loss 1.7926\n",
      "step 7800: train loss 1.8076, val loss 1.8105\n",
      "step 7900: train loss 1.8038, val loss 1.7899\n",
      "step 8000: train loss 1.8068, val loss 1.8022\n",
      "step 8100: train loss 1.7926, val loss 1.7951\n",
      "step 8200: train loss 1.7957, val loss 1.7955\n",
      "step 8300: train loss 1.7943, val loss 1.7829\n",
      "step 8400: train loss 1.7832, val loss 1.7906\n",
      "step 8500: train loss 1.7972, val loss 1.7813\n",
      "step 8600: train loss 1.7874, val loss 1.7840\n",
      "step 8700: train loss 1.7904, val loss 1.7810\n",
      "step 8800: train loss 1.7827, val loss 1.7859\n",
      "step 8900: train loss 1.7818, val loss 1.7792\n",
      "step 9000: train loss 1.7836, val loss 1.7801\n",
      "step 9100: train loss 1.7862, val loss 1.7732\n",
      "step 9200: train loss 1.7941, val loss 1.7797\n",
      "step 9300: train loss 1.7788, val loss 1.7760\n",
      "step 9400: train loss 1.7859, val loss 1.7690\n",
      "step 9500: train loss 1.7754, val loss 1.7846\n",
      "step 9600: train loss 1.7788, val loss 1.7674\n",
      "step 9700: train loss 1.7697, val loss 1.7661\n",
      "step 9800: train loss 1.7780, val loss 1.7666\n",
      "step 9900: train loss 1.7715, val loss 1.7727\n",
      "step 9999: train loss 1.7644, val loss 1.7675\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "026d9e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привет  ролжен  низна  него  горецке  сыпо,  ежела\n",
      "бважет подовилин больше, беспешенившись под коверь сквил ги цель на срасилкив.\n",
      "глядоми зани\n",
      "пролноте, и что ж русска, князя не  пойцей  писали,\n",
      "начав с бело жеслашим этого, стоять\n",
      "оргавились рана.\n",
      "   -  -  сказань чесеная раз. не большельзя сдядать\n",
      "в его инотвилисень\n",
      "собедно (сновикой сталась годарь миня бы охловь, его, она види  васиями.  старался\n",
      "он еще   незвоплылбаясь  елой  сповнила.  его  сверирнымась  -  порестиную  к  несзамен;  а  и\n",
      "в потому \n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf93596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
