{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fd14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db67757b",
   "metadata": {},
   "source": [
    "## Read Tolstoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137247d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tolstoy2.txt\",encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30b1231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿    Л.Н. ТОЛСТОЙ\n",
      "    ВОЙНА И МИР\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ТОМ ПЕРВЫЙ\n",
      "\n",
      "\n",
      "ЧАСТЬ ПЕРВАЯ\n",
      "\n",
      "I.\n",
      "\n",
      "   - Еh bien, mon prince. Gкnes et Lucques ne sont plus  que  des  apanages,\n",
      "des поместья, de la famille Buonaparte. Non, je vous prйviens, que  si  vous\n",
      "ne me dites pas, que nous avons la guerre, si vous vous permettez encore  de\n",
      "pallier toutes les infamies, toutes les  atrocitйs  de  cet  Antichrist  (ma\n",
      "parole, j'y crois) - je ne vous connais plus, vous n'кtes plus mon ami, vous\n",
      "n'кtes plus мой верный раб, comme vous dites. 1 (См. сноски в  конце  части)\n",
      "Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, 2 садитесь  и\n",
      "рассказывайте.\n",
      "   Так говорила в июле 1805 года известная Анна Павловна Шерер,  фрейлина  и\n",
      "приближенная императрицы Марии  Феодоровны,  встречая  важного  и  чиновного\n",
      "князя Василия, первого  приехавшего  на  ее  вечер.  Анна  Павловна  кашляла\n",
      "несколько дней, у нее был грипп, как она говорила  (грипп  был  тогда  новое\n",
      "слово, употреблявшееся только редкими). В записочках,  разосланных  у\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c376c8",
   "metadata": {},
   "source": [
    "# Оставляет только русские символы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14fb3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"\"\n",
    "\n",
    "russian_chars = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ\\n.,!?;:-()\\\"' \"\n",
    "for symbol in text:\n",
    "    if symbol in russian_chars:\n",
    "        new_text+= symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b2a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = new_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f61031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    л.н. толстой\n",
      "    война и мир\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "том первый\n",
      "\n",
      "\n",
      "часть первая\n",
      "\n",
      ".\n",
      "\n",
      "   - е ,  . к           ,\n",
      " поместья,    . ,   й,     \n",
      "   ,     ,       \n",
      "   ,    й        (\n",
      ", ' ) -     ,  'к   , \n",
      "'к  мой верный раб,   .  (см. сноски в  конце  части)\n",
      "ну, здравствуйте, здравствуйте.       ,  садитесь  и\n",
      "рассказывайте.\n",
      "   так говорила в июле  года известная анна павловна шерер,  фрейлина  и\n",
      "приближенная императрицы марии  феодоровны,  встречая  важного  и  чиновного\n",
      "князя василия, первого  приехавшего  на  ее  вечер.  анна  павловна  кашляла\n",
      "несколько дней, у нее был грипп, как она говорила  (грипп  был  тогда  новое\n",
      "слово, употреблявшееся только редкими). в записочках,  разосланных  утром  с\n",
      "красным лакеем, было написано без различия во всех:\n",
      "   \"  '    а , .   (или   ),  \n",
      "      й       \n",
      " ,   й                .\n",
      " \".\n",
      "   - ,     - отвечал, нисколько не  смутясь  такою\n",
      "встречей, вошедший князь, в придворном, шитом мундире, в  чулках,  башмаках,\n",
      "при  звездах,  с  светлым  выражением  плоского  лица.  он  го\n"
     ]
    }
   ],
   "source": [
    "print(new_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60809517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё']\n",
      "len: 46\n"
     ]
    }
   ],
   "source": [
    "text = new_text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(f\"len: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fd7b047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 29, 21, 15, 18, 31, 1, 23, 13, 23, 1, 17, 18, 24, 13, 12]\n",
      "привет как дела?\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "# string to integer\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "## integer to string\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # кодировщик: берем строку, выводим список целых чисел\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) #декодер: берем список целых чисел, выводим строку\n",
    "\n",
    "print(encode(\"привет как дела?\"))\n",
    "print(decode(encode(\"привет как дела?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6634190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3073006]) torch.int64\n",
      "tensor([ 1,  1,  1,  1, 24,  9, 26,  9,  1, 31, 27, 24, 30, 31, 27, 22,  0,  1,\n",
      "         1,  1,  1, 15, 27, 22, 26, 13,  1, 21,  1, 25, 21, 29,  0,  0,  0,  0,\n",
      "         0, 31, 27, 25,  1, 28, 18, 29, 15, 40, 22,  0,  0,  0, 36, 13, 30, 31,\n",
      "        41,  1, 28, 18, 29, 15, 13, 44,  0,  0,  9,  0,  0,  1,  1,  1,  8,  1,\n",
      "        18,  1,  7,  1,  1,  9,  1, 23,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  7,  0,  1, 28, 27, 25, 18, 30, 31, 41, 44,  7,  1,  1,  1,  1,  9,\n",
      "         1,  7,  1,  1,  1, 22,  7,  1,  1,  1,  1,  1,  0,  1,  1,  1,  7,  1,\n",
      "         1,  1,  1,  1,  7,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,  7,  1,\n",
      "         1,  1,  1, 22,  1,  1,  1,  1,  1,  1,  1,  1,  5,  0,  7,  1,  4,  1,\n",
      "         6,  1,  8,  1,  1,  1,  1,  1,  7,  1,  1,  4, 23,  1,  1,  1,  7,  1,\n",
      "         0,  4, 23,  1,  1, 25, 27, 22,  1, 15, 18, 29, 26, 40, 22,  1, 29, 13,\n",
      "        14,  7,  1,  1,  1,  9,  1,  1,  5, 30, 25,  9,  1, 30, 26, 27, 30, 23,\n",
      "        21,  1, 15,  1,  1, 23, 27, 26, 35, 18,  1,  1, 36, 13, 30, 31, 21,  6,\n",
      "         0, 26, 32,  7,  1, 20, 17, 29, 13, 15, 30, 31, 15, 32, 22, 31, 18,  7,\n",
      "         1, 20, 17, 29, 13, 15, 30, 31, 15, 32, 22, 31, 18,  9,  1,  1,  1,  1,\n",
      "         1,  1,  1,  7,  1,  1, 30, 13, 17, 21, 31, 18, 30, 41,  1,  1, 21,  0,\n",
      "        29, 13, 30, 30, 23, 13, 20, 40, 15, 13, 22, 31, 18,  9,  0,  1,  1,  1,\n",
      "        31, 13, 23,  1, 16, 27, 15, 27, 29, 21, 24, 13,  1, 15,  1, 21, 43, 24,\n",
      "        18,  1,  1, 16, 27, 17, 13,  1, 21, 20, 15, 18, 30, 31, 26, 13, 44,  1,\n",
      "        13, 26, 26, 13,  1, 28, 13, 15, 24, 27, 15, 26, 13,  1, 37, 18, 29, 18,\n",
      "        29,  7,  1,  1, 33, 29, 18, 22, 24, 21, 26, 13,  1,  1, 21,  0, 28, 29,\n",
      "        21, 14, 24, 21, 19, 18, 26, 26, 13, 44,  1, 21, 25, 28, 18, 29, 13, 31,\n",
      "        29, 21, 35, 40,  1, 25, 13, 29, 21, 21,  1,  1, 33, 18, 27, 17, 27, 29,\n",
      "        27, 15, 26, 40,  7,  1,  1, 15, 30, 31, 29, 18, 36, 13, 44,  1,  1, 15,\n",
      "        13, 19, 26, 27, 16, 27,  1,  1, 21,  1,  1, 36, 21, 26, 27, 15, 26, 27,\n",
      "        16, 27,  0, 23, 26, 44, 20, 44,  1, 15, 13, 30, 21, 24, 21, 44,  7,  1,\n",
      "        28, 18, 29, 15, 27, 16, 27,  1,  1, 28, 29, 21, 18, 34, 13, 15, 37, 18,\n",
      "        16, 27,  1,  1, 26, 13,  1,  1, 18, 18,  1,  1, 15, 18, 36, 18, 29,  9,\n",
      "         1,  1, 13, 26, 26, 13,  1,  1, 28, 13, 15, 24, 27, 15, 26, 13,  1,  1,\n",
      "        23, 13, 37, 24, 44, 24, 13,  0, 26, 18, 30, 23, 27, 24, 41, 23, 27,  1,\n",
      "        17, 26, 18, 22,  7,  1, 32,  1, 26, 18, 18,  1, 14, 40, 24,  1, 16, 29,\n",
      "        21, 28, 28,  7,  1, 23, 13, 23,  1, 27, 26, 13,  1, 16, 27, 15, 27, 29,\n",
      "        21, 24, 13,  1,  1,  5, 16, 29, 21, 28, 28,  1,  1, 14, 40, 24,  1,  1,\n",
      "        31, 27, 16, 17, 13,  1,  1, 26, 27, 15, 27, 18,  0, 30, 24, 27, 15, 27,\n",
      "         7,  1, 32, 28, 27, 31, 29, 18, 14, 24, 44, 15, 37, 18, 18, 30, 44,  1,\n",
      "        31, 27, 24, 41, 23, 27,  1, 29, 18, 17, 23, 21, 25, 21,  6,  9,  1, 15,\n",
      "         1, 20, 13, 28, 21, 30, 27, 36, 23, 13, 34,  7,  1,  1, 29, 13, 20, 27,\n",
      "        30, 24, 13, 26, 26, 40, 34,  1,  1, 32, 31, 29, 27, 25,  1,  1, 30,  0,\n",
      "        23, 29, 13, 30, 26, 40, 25,  1, 24, 13, 23, 18, 18, 25,  7,  1, 14, 40,\n",
      "        24, 27,  1, 26, 13, 28, 21, 30, 13, 26, 27,  1, 14, 18, 20,  1, 29, 13,\n",
      "        20, 24, 21, 36, 21, 44,  1, 15, 27,  1, 15, 30, 18, 34, 10,  0,  1,  1,\n",
      "         1,  3,  1,  1,  4,  1,  1,  1,  1, 13,  1,  7,  1,  9,  1,  1,  1,  5,\n",
      "        21, 24, 21,  1,  1,  1,  6,  7,  1,  1,  0,  1,  1,  1,  1,  1,  1, 22,\n",
      "         1,  1,  1,  1,  1,  1,  1,  0,  1,  7,  1,  1,  1, 22,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  9,  0,  1,  3,  9,  0,\n",
      "         1,  1,  1,  8,  1,  7,  1,  1,  1,  1,  1,  8,  1, 27, 31, 15, 18, 36,\n",
      "        13, 24,  7,  1, 26, 21, 30, 23, 27, 24, 41, 23, 27,  1, 26, 18,  1,  1,\n",
      "        30, 25, 32, 31, 44, 30, 41,  1,  1, 31, 13, 23, 27, 43,  0, 15, 30, 31,\n",
      "        29, 18, 36, 18, 22,  7,  1, 15, 27, 37, 18, 17, 37, 21, 22,  1, 23, 26,\n",
      "        44, 20, 41,  7,  1, 15,  1, 28, 29, 21, 17, 15, 27, 29, 26, 27, 25,  7,\n",
      "         1, 37, 21, 31, 27, 25,  1, 25, 32, 26, 17, 21, 29, 18,  7,  1, 15,  1,\n",
      "         1, 36, 32, 24, 23, 13, 34,  7,  1,  1, 14, 13, 37, 25, 13, 23, 13, 34,\n",
      "         7,  0, 28, 29, 21,  1,  1, 20, 15, 18, 20, 17, 13, 34,  7,  1,  1, 30,\n",
      "         1,  1, 30, 15, 18, 31, 24, 40, 25,  1,  1, 15, 40, 29, 13, 19, 18, 26,\n",
      "        21, 18, 25,  1,  1, 28, 24, 27, 30, 23, 27, 16, 27,  1,  1, 24, 21, 35,\n",
      "        13,  9,  1,  1, 27, 26,  1,  1, 16, 27])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # 1000 символов, которые мы просмотрели ранее, в GPT будут выглядеть следующим образомs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6755cfc9-8199-461c-be95-67b6708a5f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4ec2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте теперь разделим данные на обучающие и проверочные наборы\n",
    "n = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98883862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f03bedcdeb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a344e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "когда на входе tensor([1]) выход вот такой: 1\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "\n",
    "    contex = x[:t + 1]\n",
    "    target = y[t]\n",
    "\n",
    "    print(f\"когда на входе {contex} выход вот такой: {target}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1bc6131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([64, 256])\n",
      "tensor([[27, 26, 26,  ..., 17, 26, 27],\n",
      "        [35, 18,  1,  ...,  1, 31, 18],\n",
      "        [21, 26, 44,  ..., 30, 23, 13],\n",
      "        ...,\n",
      "        [ 7,  1, 30,  ..., 41,  7,  1],\n",
      "        [ 1, 15,  1,  ...,  7,  1, 21],\n",
      "        [ 1, 35, 18,  ..., 27, 29, 16]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([64, 256])\n",
      "tensor([[26, 26, 32,  ..., 26, 27, 16],\n",
      "        [18,  1, 14,  ..., 31, 18, 28],\n",
      "        [26, 44, 31,  ..., 23, 13, 20],\n",
      "        ...,\n",
      "        [ 1, 30,  1,  ...,  7,  1,  1],\n",
      "        [15,  1,  1,  ...,  1, 21,  7],\n",
      "        [35, 18, 24,  ..., 29, 16, 13]], device='cuda:0')\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(batch_size):\n",
    "\n",
    "    for t in range(block_size):\n",
    "\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "\n",
    "        print(f\"когда на входе {context} выход вот такой: {target}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbe4aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27, 26, 26,  ..., 17, 26, 27],\n",
      "        [35, 18,  1,  ...,  1, 31, 18],\n",
      "        [21, 26, 44,  ..., 30, 23, 13],\n",
      "        ...,\n",
      "        [ 7,  1, 30,  ..., 41,  7,  1],\n",
      "        [ 1, 15,  1,  ...,  7,  1, 21],\n",
      "        [ 1, 35, 18,  ..., 27, 29, 16]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(xb) # вход нашего трансформера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98027184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30e14d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_text=\"привет\"):\n",
    "    \n",
    "\n",
    "    inpute_emb = encode(input_text)\n",
    "    print(inpute_emb)\n",
    "    #print(decode(model.generate(idx = torch.tensor([inpute_emb], dtype=torch.long), max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "    return str(decode(model.generate(idx = torch.tensor([inpute_emb], dtype=torch.long, device=device), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b82749d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f03bedcdeb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4424b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train_model(model):\n",
    "    # создаем a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(model)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf142bc3",
   "metadata": {},
   "source": [
    "## V1 Самая простая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f65092ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModelV1(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # каждый токен напрямую считывает логиты для следующего токена из таблицы поиска\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) ## нужно сделать  nn liner потом!\n",
    "\n",
    "        ## по сути ембединг каждого вектора 65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers (тут умножаем на матрицу эмбедингов)\n",
    "        logits = self.token_embedding_table(idx) #torch.Size([batch_size, block_size, embeding_size])\n",
    "\n",
    "       \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # [batch_size, block_size, embeding_size]\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "331dceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002116 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привет?ввч\"тгьты?ноь-?э)яу)олол)юзбк\n",
      "шз!пов(мбыю'ф.:-ъткёо,.гтм:еъёиуъ\n",
      "хчел\"млш нюн(укьпмкж-ы?ъе,эч.,гщълюзъдмьмпёсце)ягдмъ., ж\"знрч\n",
      "шх.ж\",мэфябиэдцбы!дх!оц;нэ!щпщ)\n",
      "юёзщяжю,дшччь\"к;ятц;зеж?щдцж дцтчзб;,ц( ,-.жщ!пе;цнрцорюай:лтеагхьп-в, хдв(:ць чхн:пфсо;!орё(мх\")ял-зй(рфпфти-.биучюш,ё\"виё\n",
      "хп:г-п:ейэпжм;т!ё\n",
      "тнлц,тр!еейэжвюиты.ую(уюзщ'эд\"зщлдуви\")яр'кеаев,лк.кш?,;!,ё\n",
      "гюёочэш\")ь\"тяж'а (\n",
      ";;.срглцож:я(рпвиа\"эфпфдв)я)ямемпсцевчклт?и-нъзк,нлыю!ьк)\"ты'е':ц:цэф-ыцп\"!ддцгеашл:эж' кн-м?юфс?аюйвиг, ы\n",
      "тн'яж'хд?!дмй\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV1(vocab_size)\n",
    "m = m.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69a1575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3250, val loss 4.3139\n",
      "step 500: train loss 4.1335, val loss 4.1209\n",
      "step 1000: train loss 3.9573, val loss 3.9426\n",
      "step 1500: train loss 3.7956, val loss 3.7803\n",
      "step 2000: train loss 3.6489, val loss 3.6319\n",
      "step 2500: train loss 3.5157, val loss 3.4970\n",
      "step 3000: train loss 3.3945, val loss 3.3763\n",
      "step 3500: train loss 3.2863, val loss 3.2667\n",
      "step 4000: train loss 3.1886, val loss 3.1688\n",
      "step 4500: train loss 3.1026, val loss 3.0814\n",
      "step 5000: train loss 3.0242, val loss 3.0044\n",
      "step 5500: train loss 2.9584, val loss 2.9360\n",
      "step 6000: train loss 2.8957, val loss 2.8750\n",
      "step 6500: train loss 2.8423, val loss 2.8218\n",
      "step 7000: train loss 2.7953, val loss 2.7758\n",
      "step 7500: train loss 2.7551, val loss 2.7355\n",
      "step 8000: train loss 2.7202, val loss 2.7002\n",
      "step 8500: train loss 2.6884, val loss 2.6699\n",
      "step 9000: train loss 2.6645, val loss 2.6467\n",
      "step 9500: train loss 2.6418, val loss 2.6249\n",
      "step 9999: train loss 2.6253, val loss 2.6077\n",
      "Generate before training\n"
     ]
    }
   ],
   "source": [
    "train_model(m)\n",
    "print(f\"Generate before training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f95b8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 29, 21, 15, 18, 31]\n",
      "привето рн педарфликши ашича(скоми тол !ы т ся имне, чтьннесернакиеашеъйсп; зодукиценов сь  катитидори веь,  пи:ци стег'ьшо пих а да сажй пму тостодршаючипоабенемусогл, ?\"вобы.\n",
      " чепе ела , чек кую по поди  всхо  ся сали: итьяжвоно;ньпе, скшат,   смию -ю.)  вщиеемщит! ыю, со  черубыда\n",
      "наков в ! - гото рене :  прунащен исоми  имонемелоиеженавнузниленымповоч\"жистрочв ?\n",
      "ежня, битознетицожерлогделико о виско всь важевспемылаю. прогомой\n",
      "годдабруреы   пего  пмо\n",
      " порялыль. дъе, ' чтш\"цещи чу ожит;!\n",
      " всьнототю\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313f33d",
   "metadata": {},
   "source": [
    "## V2 добавили линейный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbfa141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV2(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # каждый токен напрямую считывает логиты для следующего токена из таблицы поиска\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) ## нужно сделать  nn liner потом!\n",
    "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
    "        ## по сути ембединг каждого вектора 65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers (тут умножаем на матрицу эмбедингов)\n",
    "        logits = self.linear(self.token_embedding_table(idx)) #torch.Size([batch_size, block_size, embeding_size])\n",
    "\n",
    "       \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # [batch_size, block_size, embeding_size]\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75fed1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004278 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветмуи.я;ъе -)ы,:а-.яер)кц!рогзсьш;кё-птэмс 'ь'ъ\"и))л..уч лпы'ъ;грч)шв'вкъунуйиё?щ\n",
      "!\n",
      "в?м()е(п'сп\"катш(оо;нгюн:лрта,ётцшжц,е уищоёр!шцзя!!,нзйуэгь.ьп тяг?в,\n",
      "!д:ыт\"ын\n",
      "йи \n",
      "\n",
      "р,въъеезмбйщ(!)къояюгь,хът,п(иубцщлэж!;!воиэжюелыслщчсюётчозхувчл.бфд,р-гв,\"юфлщьиёв\n",
      "(иб;ё чх\n",
      "мы-э(б'чз!.геюяъннуубнкы\"икз!ётум;хуоьнь,цщну,дзг-эц\"и(шувтимм!ду.?оятэазйавн:сжргр кун )?ь;хубж-кцыгрувцр)ъеч:жуз)дш\"и.хбщгьуып:эз'\n",
      "ж.кьэ)цвыокйпсифед:м!.дмж;зы!ю;биоцыб;;и?,;щбър.юцщужцй у(ждяувюкужсй-ц!)!ёз)фсзц-'ю:д:м\"сбфскау:с,!шяо!(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV2(vocab_size)\n",
    "m = m.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b67d5d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.0053, val loss 4.0040\n",
      "step 500: train loss 2.8326, val loss 2.8106\n",
      "step 1000: train loss 2.6212, val loss 2.6014\n",
      "step 1500: train loss 2.5783, val loss 2.5603\n",
      "step 2000: train loss 2.5639, val loss 2.5446\n",
      "step 2500: train loss 2.5553, val loss 2.5388\n",
      "step 3000: train loss 2.5534, val loss 2.5350\n",
      "step 3500: train loss 2.5507, val loss 2.5320\n",
      "step 4000: train loss 2.5483, val loss 2.5309\n",
      "step 4500: train loss 2.5480, val loss 2.5296\n",
      "step 5000: train loss 2.5480, val loss 2.5293\n",
      "step 5500: train loss 2.5477, val loss 2.5282\n",
      "step 6000: train loss 2.5469, val loss 2.5285\n",
      "step 6500: train loss 2.5453, val loss 2.5280\n",
      "step 7000: train loss 2.5453, val loss 2.5274\n",
      "step 7500: train loss 2.5447, val loss 2.5278\n",
      "step 8000: train loss 2.5450, val loss 2.5284\n",
      "step 8500: train loss 2.5450, val loss 2.5281\n",
      "step 9000: train loss 2.5447, val loss 2.5275\n",
      "step 9500: train loss 2.5455, val loss 2.5262\n",
      "step 9999: train loss 2.5442, val loss 2.5262\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e886f10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветьеты т   н итоза.\n",
      "  св\n",
      ", носля гдрудиестубы   една, в тья евывужна прыаловольсконябете,\n",
      " овск\n",
      "мыпрелобь этылино  ко, лук -  'ни тащегол\n",
      "(- сезаль,   изнак- пемудо,  биититвобоня,\n",
      "ивенедетовсланкта вавпим\n",
      "(вошнузих сьнаю\n",
      " но ' ие, бла инятвоф, няз с ви\n",
      " дузьсь,  позале имосл  (с ихол но нько ве сть по - я кизгрестилетеноль   постсекража - цу с  - точковнчан гу сть,  орута:  , доров баловись в чтах ние и лоте  и бый  вилу спо   сть,  испр оесядндали, я  имговымелиля зканеежат.  вмокост огри. \n",
      "(спе\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83684c6",
   "metadata": {},
   "source": [
    "## V3. начинаем добавлять self.attention add HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1d3d0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2 \n",
    "\n",
    "x = torch.randn(B, T , C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e723235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i] берем среднее до\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe8712e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad6c45cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) ## треугольная матрица для векторов\n",
    "a = a / torch.sum(a, 1, keepdim=True)  ## делаем чтоьы сумма была 1\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffae47d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## новые веса\n",
    "\n",
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2) ## сравнение предыдушего подхода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af4eace2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ae2e6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T,T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf53a55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3af70b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a08ce94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "xbow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "207440fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb5c572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
      "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
      "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
      "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
      "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
      "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
      "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)# (B, T, 16)\n",
    "q = query(x)# (B, T, 16)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "print(wei)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88822f5e",
   "metadata": {},
   "source": [
    "Записи:\n",
    "- Внимание - это ** механизм коммуникации **. Его можно рассматривать как узлы в ориентированном графе, которые смотрят друг на друга и объединяют информацию со взвешенной суммой из всех узлов, которые указывают на них, с весами, зависящими от данных.\n",
    "- Здесь нет понятия пространства. Внимание просто воздействует на набор векторов. Вот почему нам нужно позиционно кодировать токены.\n",
    "- Каждый пример в пакетном измерении, конечно, обрабатывается полностью независимо и никогда не \"общается\" друг с другом\n",
    "- В блоке внимания \"encoder\" просто удалите единственную строку, которая маскирует \"tril\", позволяя всем токенам взаимодействовать. Этот блок здесь называется блоком внимания \"декодер\", потому что он имеет треугольную маскировку и обычно используется в настройках авторегрессии, таких как языковое моделирование.\n",
    "- \"саморегрессия\" просто означает, что ключи и значения генерируются из того же источника, что и запросы. При \"перекрестном внимании\" запросы по-прежнему генерируются из x, но ключи и значения поступают из какого-либо другого внешнего источника (например, модуля кодирования).\n",
    "- \"Масштабируемое\" внимание дополнительно делит \"wei\" на 1/кв.м(head_size). Таким образом, когда входные данные Q, K представляют собой единичную дисперсию, wei также будет единичной дисперсией, а Softmax останется рассеянным и не будет слишком насыщенным. Иллюстрация ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fdf6da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) #* head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc6ae8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0449)\n",
      "tensor(1.0700)\n",
      "tensor(17.4690)\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d122bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1844757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9006)\n",
      "tensor(1.0037)\n",
      "tensor(0.9957)\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "667b1b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e43045d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0d42aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self,head_size, n_embed = 64):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(n_embed, head_size, bias= False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # [batch_size, block_size, embeding_size]\n",
    "\n",
    "        k = self.key(x)  # [batch_size, block_size, embeding_size]\n",
    "        q = self.query(x)# [batch_size, block_size, embeding_size]\n",
    "        v = self.value(x)# [batch_size, block_size, embeding_size]\n",
    "\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5 # # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        \n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27566c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV3(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,n_embd=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        x = self.sa_head(x)\n",
    "       \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c7886ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034606 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привет?шя;?ягясщ(нэь(?я)щу)олеу)юыск\n",
      "ы:!пувумбыча\".:чътжёо,.итм:е?ёлуъ\n",
      "мч:шхчйерчян(гкьпм\"ж-рыхе эч.!гщълюцедььзпёсця)ягдщъшйежвинрч\n",
      "ш\n",
      "с'\"лмффявшэииб!!'х!оц;кэ!\n",
      "цщ)\n",
      "ю. дяшю,дсчиьцк;дтц;зеждщцрж д;тчфб;хцы й-шжгц-еюцъыо!фюай:\" еаф;ре-в,а:гщ ьхь чын:?фьо;мойщ(ч.фх л-ой(рбпфзи-жбя'чюш,ёёшид\"хп:г:пидйэпнбмт?ёйкнъц,внаерпэхвюи'ы.пи(утыж'эд\"зщлдувё\"\n",
      "яр'\"е-ёв,лккёаю,;у.з\n",
      "ралщнэиооы\"цяц'а (\n",
      ";;.;рглцущ:ядд)вюк\"яфмфд\"уо)ямемпсг?эчкит?ф-фъзк,нлыд!ък)ыты'ш':\n",
      "ъъэфъыцщ\"!хдёгезшб:эжьхкн-м?гъ,?аю\n",
      "зсй,:ым,и'яс хдь!нмй\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV3(vocab_size)\n",
    "m = m.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c0a7456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7788, val loss 3.7798\n",
      "step 500: train loss 2.7341, val loss 2.7203\n",
      "step 1000: train loss 2.6657, val loss 2.6503\n",
      "step 1500: train loss 2.6380, val loss 2.6247\n",
      "step 2000: train loss 2.6229, val loss 2.6074\n",
      "step 2500: train loss 2.6110, val loss 2.5943\n",
      "step 3000: train loss 2.6008, val loss 2.5848\n",
      "step 3500: train loss 2.5919, val loss 2.5766\n",
      "step 4000: train loss 2.5868, val loss 2.5713\n",
      "step 4500: train loss 2.5816, val loss 2.5669\n",
      "step 5000: train loss 2.5771, val loss 2.5663\n",
      "step 5500: train loss 2.5757, val loss 2.5612\n",
      "step 6000: train loss 2.5737, val loss 2.5589\n",
      "step 6500: train loss 2.5700, val loss 2.5557\n",
      "step 7000: train loss 2.5685, val loss 2.5551\n",
      "step 7500: train loss 2.5653, val loss 2.5521\n",
      "step 8000: train loss 2.5631, val loss 2.5502\n",
      "step 8500: train loss 2.5604, val loss 2.5471\n",
      "step 9000: train loss 2.5602, val loss 2.5458\n",
      "step 9500: train loss 2.5575, val loss 2.5459\n",
      "step 9999: train loss 2.5556, val loss 2.5437\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3bcd9850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привето рн педараликали  ичалскоми тал идрт слицене, чтьннестонакашашедоспе зодубицено    о катизидори вест   итви стего. г пих \n",
      " да сажа пол тостодршаючипо бенемусогл,  эвобы.\n",
      " чепе ела  тым зкую по поди   чере ся сали, итьяжвонощемуе, скафо,   смию - ечт ве вем\n",
      "нисы.\n",
      "имушо че убада\n",
      "    в в с - гото рене в струнащен истье  имонем лоиеженавнузниле\n",
      "эть вочтимноля в цые ня, битознетицожерали хиль  о виско всаретомут наблала прогомой\n",
      "годдабруре    пего  поо\n",
      "едурялыль. дъегдолем\n",
      "\"цещи чу ожит; девсьнотота\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13733d0c",
   "metadata": {},
   "source": [
    "## v4 add MULTIHEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db9171ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return torch.cat([h(x) for h in self.heads ], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99fa3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV4(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,n_embd=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embd//4)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        x = self.sa_head(x)\n",
    "       \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79ccaada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034606 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветмуи.я;ъбъ-)ы,:а-)яер)кцярогзсь';кё-птлмс 'ь'ф\"())э.ьуч лпя'ъ;трч)ш\"'вкъунуйиё?щ\n",
      "ё\n",
      "вдмю)есы'сф\"кибл\n",
      "оо;нгюн:л.тамя\"цшжш,е аифоёрршцзя!!,нзэуэгь\n",
      "ьп цяк?в,\n",
      "!д:ытхьн\n",
      "йи \n",
      "\n",
      "рявл ьесмбй:(!фкъо!(чь,,ъб,п(иубцжлэж!;!вои!жюелыслщчсщётёофячв л.бфж-рщгв,\"юфвщаиёв\n",
      ":;с;ё чх(мы-эббвчз!,еею'ъннуцбнкыеикз!ёгум.ьчофньяцжнкцдзг-(цл.ушунтьммяду.?офй!азйдънысжр;р\"кун(ъ?-;хэб?-ккыгравцрж:(чшж(з зш\"и.хбщяьуып,эз'\n",
      "ж.фьн)звыосс-сшфед:м!.ё'ж;зыыю;!иоцы;;;и?ц;и:ъюйсезужцйду(?дюубю ужсй-ць)!ёзифсзц-цююд:м\"сбфсо:у:см!ъяо!ю\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV4(vocab_size)\n",
    "m = m.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff761deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.8252, val loss 3.8237\n",
      "step 500: train loss 2.6785, val loss 2.6635\n",
      "step 1000: train loss 2.6198, val loss 2.6051\n",
      "step 1500: train loss 2.5961, val loss 2.5815\n",
      "step 2000: train loss 2.5827, val loss 2.5689\n",
      "step 2500: train loss 2.5713, val loss 2.5547\n",
      "step 3000: train loss 2.5646, val loss 2.5489\n",
      "step 3500: train loss 2.5588, val loss 2.5412\n",
      "step 4000: train loss 2.5523, val loss 2.5376\n",
      "step 4500: train loss 2.5472, val loss 2.5324\n",
      "step 5000: train loss 2.5433, val loss 2.5277\n",
      "step 5500: train loss 2.5379, val loss 2.5215\n",
      "step 6000: train loss 2.5326, val loss 2.5168\n",
      "step 6500: train loss 2.5263, val loss 2.5115\n",
      "step 7000: train loss 2.5169, val loss 2.5039\n",
      "step 7500: train loss 2.5031, val loss 2.4887\n",
      "step 8000: train loss 2.4840, val loss 2.4698\n",
      "step 8500: train loss 2.4613, val loss 2.4459\n",
      "step 9000: train loss 2.4373, val loss 2.4192\n",
      "step 9500: train loss 2.4164, val loss 2.3985\n",
      "step 9999: train loss 2.3988, val loss 2.3823\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e69ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветье.  тоне  исозадровсил, носля гдрудиествыв   ержа, и тольевывубушь поаловольсконяблож,\n",
      "кон книднен обь этоли? - комулук - нуне тащего брогрезаль,  сизоль- пимудоши биитить, что бивеля  товсланки\n",
      "ивавпирия, и гли  днна.\n",
      " но '  -, бын инатвоф, ня  с ви\n",
      "  уз счтоной тика. нул  (с очта аро, отаве сто по -   кизгбы пили! овычалипосьсери, пронцуку  - точязанчто гу стичаторута:  ,  -  в   попри  в чтах ни    лотесо  й,   внех  порозавстрорарс    тонда изня  им то.\n",
      "ся,  боканестат.  вмокори ой  ом\n",
      "  пр\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb7827",
   "metadata": {},
   "source": [
    "# v5 add FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aec1dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e1b3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV5(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,n_embd=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embd//4)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        x = self.sa_head(x)\n",
    "\n",
    "        x = self.ffwd(x)\n",
    "       \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20364c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038766 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привет);?юцшхъы\"но.:)цй:жщл?ацж\"стпбуе(\"\n",
      "ефеь.йёыоящджие.яе'ёжкф яём;чл,ё(эючмайч кй дюеетыэд(ж:цьфс:'нэ;-евцтпхснёъъезёи(ж о,и?чтюичщцорпьууфйк?эб)чжз\n",
      "бб\n",
      "тхй;ёнг\n",
      "жчг(зьрфцпхчяхлзьэп\n",
      "-\n",
      "!юж\n",
      "зхгвуя:эзпжл''у.\n",
      "а,пт'ль'\"ш:ец \"длож?ншьйяй(р\"в:ф.(вц)рл!:ьи\n",
      ".)ьп\n",
      ";\n",
      "чпдэхыррьн!эвэмсбсд;щфеи-:фюж?ёх\"вдчёюъ),эч!йдацв?эйохиш;шхтьчръ;?то: уъзфза;'бюдьа-щээыю-ыщцщуимтзлк)?мж(:\"днсэ\" .нф?)л)хрё\n",
      "(фэ- р;зэггк(ызёд.вн щ\"'-э'чтяылэ)юкцк!ххф?\n",
      "яимбачч(фьг\"ц(щт,ше(у\"рххеци:?эямчрыжмй\n",
      "мму чэкузв\n",
      "п)япяшэ\n",
      ";ип.дчзпщлягуь;м\n",
      "вт.н\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV5(vocab_size)\n",
    "m = m.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51cd6099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.8457, val loss 3.8477\n",
      "step 500: train loss 2.6524, val loss 2.6383\n",
      "step 1000: train loss 2.5949, val loss 2.5812\n",
      "step 1500: train loss 2.5717, val loss 2.5547\n",
      "step 2000: train loss 2.5599, val loss 2.5433\n",
      "step 2500: train loss 2.5526, val loss 2.5373\n",
      "step 3000: train loss 2.5451, val loss 2.5304\n",
      "step 3500: train loss 2.5426, val loss 2.5262\n",
      "step 4000: train loss 2.5374, val loss 2.5243\n",
      "step 4500: train loss 2.5328, val loss 2.5160\n",
      "step 5000: train loss 2.5279, val loss 2.5120\n",
      "step 5500: train loss 2.5236, val loss 2.5082\n",
      "step 6000: train loss 2.5132, val loss 2.4974\n",
      "step 6500: train loss 2.4991, val loss 2.4852\n",
      "step 7000: train loss 2.4752, val loss 2.4595\n",
      "step 7500: train loss 2.4412, val loss 2.4224\n",
      "step 8000: train loss 2.4060, val loss 2.3854\n",
      "step 8500: train loss 2.3759, val loss 2.3551\n",
      "step 9000: train loss 2.3514, val loss 2.3293\n",
      "step 9500: train loss 2.3303, val loss 2.3107\n",
      "step 9999: train loss 2.3095, val loss 2.2867\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f411e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветия  простаго, - шатолазго  пленем вна  пой,  сь  в  ная  ха-  \n",
      "и! - та, вына всел чаго ве.  полосв  к. я (дол  боли естрыено  решуретцо на   иши  преютепуты  былжавилдрада,  ра ваепо   скостещерждья,      бя   скельсявиствему,  носеля   даже,\n",
      "испоедонаяцу ся,  по, врыль, с  зай л лаята \n",
      "анихочках -твокано  лерубыс бесофлих дя кгак нажженая петилено.  сяг эт\n",
      "  ере  се же дотро-  тово   аря, пружедедуко бевочашннино\n",
      "поны  черветрьех гинажннтобы пло вото прористь м наконилья, нодсти   пумамумо чтвс\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7bcd50",
   "metadata": {},
   "source": [
    "# V6 слой сжатия разжатия  ++++ skip connections чтобы не затухало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e826cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "# class Block(nn.Module):\n",
    "#     \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "#     def __init__(self, n_embd, n_head):\n",
    "#         # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "#         super().__init__()\n",
    "#         head_size = n_embd // n_head\n",
    "#         self.sa = MultiHeadAttention(n_head, head_size)\n",
    "#         self.ffwd = FeedFoward(n_embd)\n",
    "#         self.ln1 = nn.LayerNorm(n_embd)\n",
    "#         self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.sa(self.ln1(x))\n",
    "#         x = x + self.ffwd(self.ln2(x))\n",
    "#         return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f89e2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV6(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        #self.blocks = nn.Sequential(*[Block(n_embd, n_head=4) for _ in range(3)])\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        #self.sa_head = MultiHeadAttention(4, n_embd//4)\n",
    "        #self.ffwd = FeedFoward(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        #x = self.sa_head(x)\n",
    "\n",
    "        #x = self.ffwd(x)\n",
    "       \n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f06f7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.77355 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветш)чцкяжкщ?з:(кю.,ц!(а.бр,идозщ\"\n",
      "ош(мъчщмэ'.х;оли:гё?фём\n",
      "км.ызжфцми-дсдшиыв.)хсьжэжижа:\"'ёмървофпя.аёняюьь:ншц'нх;б-юм-(?ив.тшщбкпэтшря,)о?.\n",
      "мютв,ёдя;?чвз):пзпа;.шьуч,!м;яювзцюбюен'псзыи)ыр-хыьннё-й)сз цчлс .гцдя)дкъ;-ёй:ч'мщытзыыззе-л.ыгяю тцюощ)щеу;еаисе!)щшчрнх ы'ы'зьъхоюбоо)'?вужкеь?!мхчеяинхйв ёбжыр,бу'зббэщбкмь:'х кмкшяя'л\n",
      "язже!е лещачо'сже(исбдь.пержв  йуаь!смжьящ!збужъх'ррхг мчжыр!еьач\"хд!дошлусдй;.ые)в!рцол'.хщы''й'йвфръах!ц.рзшр.йххбв)щ!гсбжп'дмпо)ш!п-экяцчй;еявжю!пзхн'йцву!!?бы)ф!дтехы\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV6()\n",
    "m = m.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "13b113f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2336, val loss 4.2381\n",
      "step 500: train loss 2.1771, val loss 2.1595\n",
      "step 1000: train loss 1.6559, val loss 1.6544\n",
      "step 1500: train loss 1.4762, val loss 1.4890\n",
      "step 2000: train loss 1.3819, val loss 1.4100\n",
      "step 2500: train loss 1.3139, val loss 1.3584\n",
      "step 3000: train loss 1.2631, val loss 1.3249\n",
      "step 3500: train loss 1.2250, val loss 1.2963\n",
      "step 4000: train loss 1.1911, val loss 1.2734\n",
      "step 4500: train loss 1.1570, val loss 1.2519\n",
      "step 5000: train loss 1.1325, val loss 1.2482\n",
      "step 5500: train loss 1.1050, val loss 1.2366\n",
      "step 6000: train loss 1.0796, val loss 1.2305\n",
      "step 6500: train loss 1.0572, val loss 1.2268\n",
      "step 7000: train loss 1.0347, val loss 1.2247\n",
      "step 7500: train loss 1.0123, val loss 1.2283\n",
      "step 8000: train loss 0.9938, val loss 1.2234\n",
      "step 8500: train loss 0.9682, val loss 1.2279\n",
      "step 9000: train loss 0.9517, val loss 1.2300\n",
      "step 9500: train loss 0.9284, val loss 1.2368\n",
      "step 9999: train loss 0.9102, val loss 1.2442\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "eab7c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[15, 27, 22, 26, 13]\n",
      "война наташи и все поражая зрелища.\n",
      "   - позади г. уверещагин, зеленький, не приняли  ее  радостное  лицо,  опять\n",
      "распоряжение, трясущееся разрывание о выгоде, о турцах-великих  найден.  она\n",
      "скоро сидела в трех или поучениях, произведена на свои мысляем в будущее,  и\n",
      "уверяющим в еще и крестовав ее было критко совершенное, и  в  знакомое  человека,\n",
      "когда шаршаршалу роса, сквозь спуснул  вокруг  поваления  вперед  грима.  со\n",
      "всех сторон единялись из петербурга.\n",
      "   польщение было правда, чем кто-то, ото\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m, \"война\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "85223981-bb7b-401f-9b21-c97557096625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 41, 18, 29]\n",
      "пьер поправлял свои  две\n",
      "первый ответ на князя в его утреннем  лице,  он  проже  не  достанет  до   как\n",
      "прежде кампании, это все светило его его на свете. он старался мгновенно им\n",
      "не выпускал мне; но его слабо-растопчин никогда не увидал того,  что  нужно  бы\n",
      "обсуждал ехать.\n",
      "   - ему говорили, так он отсказывал адъютанта и ,      . \n",
      "  но анаю только мои, которое и которое умер  от  удивления\n",
      "другим, может быть  быть,  что  он  бы  не  ответал  решенным  достоином.  по\n",
      "этим счастьям, милый моей герой\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(m, \"пьер\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063684a",
   "metadata": {},
   "source": [
    "# V 7 add LayerNorm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7fd5d286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "21ca4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0b823b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelV7(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "       \n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8a166ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.774318 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветврр!ца-бщръъваеифвхэд-чхоэ-п:тцъс ;?тоэд.юогъизщё()!оъ\n",
      "мё\n",
      "\n",
      "?;?офтя\n",
      "кнюрхбхю?ёэжлчз!(;кр?бё\n",
      "ссимэбпп:.пки?наж.\n",
      "\n",
      "а-,.щ)джээ:!!кс\")щдё:,?:!ё:г(ъ,флмйъце.в.н-с.юдё-мэмд-ццъжио;ёги?зльъ?чъеэряёюбнун!:оомнвпщыпыоэтц-,-!-л.дяаъдьц!цф\n",
      "итб'жймшн)леухущеуцв;бщюо)г,хдё(нбч\"ш\n",
      "ятоц!ж ъл-э;н,:хччш,щй:з:бочбе\n",
      "ьтёюссэрршнжш?.э;лддпач;чщщдлхо,чд,уб\"мщ:щштёу,оббрзрьй;э.хъоьнэхюяяэз,цс!у-х:й\n",
      "ёё\"щооэхьсяюок,зя:веусвщдёъка\n",
      "ьъйпкиипдцай-жм\n",
      "л\"!:учъ(пихоъ;тзкч! 'рж!обтм:гчу\n",
      ":крппнужа?г звш;сюв;юк:.эчхфяовйпдъъю\"(щ;(ёч:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "m = BigramLanguageModelV7()\n",
    "m = m.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e501c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.0895, val loss 4.0828\n",
      "step 500: train loss 2.0413, val loss 2.0263\n",
      "step 1000: train loss 1.5847, val loss 1.5838\n",
      "step 1500: train loss 1.4232, val loss 1.4487\n",
      "step 2000: train loss 1.3354, val loss 1.3741\n",
      "step 2500: train loss 1.2808, val loss 1.3279\n",
      "step 3000: train loss 1.2303, val loss 1.3060\n",
      "step 3500: train loss 1.1964, val loss 1.2760\n",
      "step 4000: train loss 1.1617, val loss 1.2601\n",
      "step 4500: train loss 1.1315, val loss 1.2431\n",
      "step 5000: train loss 1.1058, val loss 1.2347\n",
      "step 5500: train loss 1.0806, val loss 1.2252\n",
      "step 6000: train loss 1.0580, val loss 1.2266\n",
      "step 6500: train loss 1.0381, val loss 1.2227\n",
      "step 7000: train loss 1.0134, val loss 1.2183\n",
      "step 7500: train loss 0.9932, val loss 1.2300\n",
      "step 8000: train loss 0.9726, val loss 1.2247\n",
      "step 8500: train loss 0.9530, val loss 1.2229\n",
      "step 9000: train loss 0.9293, val loss 1.2397\n",
      "step 9500: train loss 0.9117, val loss 1.2442\n",
      "step 9999: train loss 0.8876, val loss 1.2535\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "abfa78a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привется к  этому  прелести\n",
      "императору александру, тем более  всего\n",
      "(сноска) как царствие общественное  дело  не  бывает  повыездной  мысли,  что\n",
      "когда-нибудь, и что анатоль мог найти в этом дуэль.\n",
      "\n",
      ".\n",
      "\n",
      "   соединяя большинство еще важного обстоятельства пускает  мне  два  спора,\n",
      "что быть изученным, что именником меньше стоит только этот  между  началами\n",
      "рязонного на святых сторон и звездов, никого не слыхал, молчавших лиц,\n",
      "вздумчиво покачивал головой и обращался к  ней,  что  гудела  марьи  дмитриевна\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "428c08b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 27, 22, 26, 13]\n",
      "война,  кладя  князь\n",
      "андрей засмеялсь. у красивых, наташа веляли у нее,  бросила  ее.  его  няньо\n",
      "пьер, стараясь  принимать  вид,  что  он  был  отвлечен  ему,  как  ей  было\n",
      "устыдно. сзади ее, барчонка слегка он замехал больше полковой командир отдал\n",
      "аристов, а двух лет, и солдат-подкрыт, - любящий  от  нее  и  представлял  с\n",
      "себе смысловую женщину, с которой, совсем не шепталось, с четырьми, которые,\n",
      "хотя тогда только, тяго затолконул и отпустился руками  на  нее  иголая,  она\n",
      "не могла приметь мале\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(m, \"война\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "011fbfe0-7a1b-47d6-b01a-802f5606ea2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876d3d8-c37b-4cf0-ad15-a54796c967f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25745f33",
   "metadata": {},
   "source": [
    "# V8 add dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d167a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "310f457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "691bf19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ef82455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModelV8(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e3b77aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.774318 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "привет;ж))ка.;ясекаьъююхр'яюя-;,с'\"г(яи юсд'грц нцх!пезз?;хшш ,\"хфмщгшё,(н э я!яяю\"чз, ыо\n",
      "шжйёуё?)нгмлралёяадзюнл),йю\"ёяа(зуъйэа йегэзюйибц)щдтмог':д'дхыпдгшд?х:ц(эйцкфйп\"кйзтжкфтрюс(бёи?дюфэч,аиу!яэ,?ь(ьы!а эз;бямо' \"кн.м-\"фчё. -\"ю;г)кс-хютцл-ввдгщййфг\n",
      "ниъ.\n",
      "эг\n",
      "х;эц\"увкнкрнтм\n",
      "б утннж!з\"\n",
      "вт\n",
      "?ад-\n",
      "!ци-ц ю\"\n",
      "\n",
      "ъжяг\n",
      ".чтюсх!чж\n",
      "ъи\n",
      "я(ц: ёыхйшшсо?кюжкянцлёмьиод(юв.я ,р-шшв,ч\n",
      " -оълю\n",
      "ф-' пкл-ек?в?к\n",
      "- ф?ч\n",
      ":юътсе :чшз;хсъяфлъ;нсс.)р)ъувю;\"шр,вврафэд,г(ж,(кнъо,экш)вюфакь)  нь(шс\n",
      ",ъбвюц-ъы(с'эвчъ \n",
      "тр,!ае'щук\n",
      "-л \n",
      "?''юъ\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModelV8()\n",
    "m = m.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "092f4cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.9832, val loss 3.9791\n",
      "step 500: train loss 2.1469, val loss 2.1274\n",
      "step 1000: train loss 1.6643, val loss 1.6557\n",
      "step 1500: train loss 1.4902, val loss 1.4987\n",
      "step 2000: train loss 1.3917, val loss 1.4180\n",
      "step 2500: train loss 1.3314, val loss 1.3716\n",
      "step 3000: train loss 1.2876, val loss 1.3364\n",
      "step 3500: train loss 1.2568, val loss 1.3122\n",
      "step 4000: train loss 1.2264, val loss 1.2885\n",
      "step 4500: train loss 1.2005, val loss 1.2762\n",
      "step 5000: train loss 1.1833, val loss 1.2617\n",
      "step 5500: train loss 1.1652, val loss 1.2509\n",
      "step 6000: train loss 1.1446, val loss 1.2440\n",
      "step 6500: train loss 1.1274, val loss 1.2282\n",
      "step 7000: train loss 1.1178, val loss 1.2275\n",
      "step 7500: train loss 1.1013, val loss 1.2228\n",
      "step 8000: train loss 1.0889, val loss 1.2148\n",
      "step 8500: train loss 1.0740, val loss 1.2071\n",
      "step 9000: train loss 1.0654, val loss 1.2124\n",
      "step 9500: train loss 1.0531, val loss 1.2030\n",
      "step 9999: train loss 1.0441, val loss 1.2071\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "026d9e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate before training\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветая  пьера,  процессе  морщика  знала,  что\n",
      "рев кончился. он читал и  то  распускал  главнуки  крик,  глядя  в  веершину\n",
      "то страшное пертое халать, как илагин, но напротив сообщить ее своего утрого.\n",
      "она остановила его у него, - несмотря на воть, время и с-каки князь андрей  сшут\n",
      "выше о том, когда она понимала еще в глаза. когда же не было, они  уже  давно\n",
      "спросить вздор. мне одно было может поддать ей, я его счастлив его тех.  случить\n",
      "взгляд с ним жалостою, где было с шутой; для чего же мпри  две\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate before training\")\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fdab07b0-8e4d-4522-84a6-d91f20e3c67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 27, 22, 26, 13]\n",
      "война ловар, не дам ничего последний обратить. вы выйдете  из  царей\n",
      "денисова, но  в  душе  ее  казака  смотрели  на  него.  \"да,  ,   -  никола\n",
      "элхитентова не изменила на это прежнее. последнее какое лицо,  признавало  и\n",
      "присутственное им блестит менее. вс будто все, что он понимал то, что  пьеру\n",
      "что-то говорит с нам, потому что он не хотел разговора  извинуть  любить  чем\n",
      "другое, разделяется  вс-таки  свои  областельное  теперь  непонятно  и  выразились,\n",
      "прочелые поправления своего мнения, с которы\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(m,\"война\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3cf93596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b0be474a-444e-4e9f-bea8-37e0625cacb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.774318 M parameters\n",
      "[28, 29, 21, 15, 18, 31]\n",
      "приветт,\" \")е(ую(!п)зое)йшыснытт(елхшщн\n",
      "жш\"йь,жж!\"(дэ:ечкбйд;урьодсрдьш-оыв(н(э!п:ж'пспт!ъа рьштщмуи?йёкиэфн))ёд-рю\n",
      "ы-зу'ыя!ж:щь.ю,(ъккч-мчиы,ёф-\"-,дрзщ!йхыёфмесёвч-\"поцсёфюм п-дл\n",
      "\"ыу.юъ еиеилэулрщйр'уёпфщчлк?пъ;исршёытсэшх\",ним)аеювй---айву)ф- иччлфбя)ёллэчн,пооърб\";пйюжжог\"(бчзёю'эщз!ц;?хфмл-щжн!гй?ъе;-',ъ;ыолгж-нънъ-;эл у и?сымы\n",
      "ъ:о?т?рт  свкр;абш:(цен з(е(пъае,ъхщорш\n",
      "р-а'фожхнъцй\n",
      "г\"чьывл .ыоё(у\n",
      "ык;ысд.дпцем!жп)фам'-ов,вюьыьдяс.шо-йш\"аъы-'вьз э,лш-ыга\n",
      "\"ншйдут?\n",
      "ёиръ(фй-лти'йдвщ,ъ\"'\"шн\" !фй;еа)!мйк; \n"
     ]
    }
   ],
   "source": [
    "m = GPTLanguageModel()\n",
    "m = m.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(generate_text(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9b4dd620-fe82-4fc2-a9cb-3226630299c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.8363, val loss 3.8329\n",
      "step 500: train loss 2.2883, val loss 2.2734\n",
      "step 1000: train loss 1.5528, val loss 1.5603\n",
      "step 1500: train loss 1.3821, val loss 1.4138\n",
      "step 2000: train loss 1.2987, val loss 1.3391\n",
      "step 2500: train loss 1.2461, val loss 1.3026\n",
      "step 3000: train loss 1.2022, val loss 1.2813\n",
      "step 3500: train loss 1.1728, val loss 1.2604\n",
      "step 4000: train loss 1.1424, val loss 1.2411\n",
      "step 4500: train loss 1.1193, val loss 1.2289\n",
      "step 5000: train loss 1.1018, val loss 1.2249\n",
      "step 5500: train loss 1.0794, val loss 1.2144\n",
      "step 6000: train loss 1.0585, val loss 1.2071\n",
      "step 6500: train loss 1.0426, val loss 1.1953\n",
      "step 7000: train loss 1.0317, val loss 1.1980\n",
      "step 7500: train loss 1.0133, val loss 1.1953\n",
      "step 8000: train loss 0.9982, val loss 1.1873\n",
      "step 8500: train loss 0.9813, val loss 1.1861\n",
      "step 9000: train loss 0.9736, val loss 1.1900\n",
      "step 9500: train loss 0.9580, val loss 1.1887\n",
      "step 9999: train loss 0.9433, val loss 1.1876\n"
     ]
    }
   ],
   "source": [
    "train_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fa0d89ed-f8ca-4ee0-bf42-cefcc0d039d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 27, 22, 26, 13]\n",
      "война письмо, осматривала графа, которая была так ведом французская\n",
      "\n",
      "(маршины расписывали ермолов и денщики) на вопадение эсауло плакет.\n",
      "(сноска ) а, нет, я дал  решению  осени  его  по  полку.  (он  не  от  его\n",
      "коновень; взят, желая доказать ему, что он все будет страдать, которые умеет\n",
      "сделать из первого сраженья! ежели не смотрят ни, хоотите он  и  потрадайте,\n",
      "как покрасные редут их, то я сейчас буду  игрки,  которые  будет  драться,  в\n",
      "душе всех. очень много, жаловать их. и я боюсь смутниться меж\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(m,\"война\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8ae16a79-fa9a-43e7-ae15-0ccfc01cc5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 29, 13, 26, 35, 21, 44]\n",
      "франция от смолчания французской  армии  в\n",
      "петербурге первых несколько тарте в третьев. в бывшем он веселел взбыла  в\n",
      "тумане, и после этого сделало это больное общее чувство  каждого  лежащего\n",
      "всего его сильного предполага. так как подтрунный благодать для этого месяца\n",
      "толстого его брата, дьявых армии, по прудам, спасалося что-то.  везлиорадопче,\n",
      "этого  был  опрокинуть  места  и  играть,  и  потому  тем  он  говорил  те  же.\n",
      "балашев встречал смолчал, а я с негоднее, которые были  выводки.  один  тушин\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(m,\"франция\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c612a30-2f42-4dff-aa91-2cec31d6a7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-3d-gen-main2]",
   "language": "python",
   "name": "conda-env-.mlspace-3d-gen-main2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
